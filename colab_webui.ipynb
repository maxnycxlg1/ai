{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxnycxlg1/ai/blob/main/colab_webui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "环境配置 environment"
      ],
      "metadata": {
        "id": "_o6a8GS2lWQM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9b7iFV3dm1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c51cdf6b-854b-43cd-c381-c590a11649e9"
      },
      "source": [
        "!pip install -q condacolab\n",
        "# Setting up condacolab and installing packages\n",
        "import condacolab\n",
        "condacolab.install_from_url(\"https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh\")\n",
        "%cd -q /content\n",
        "!git clone https://github.com/RVC-Boss/GPT-SoVITS\n",
        "!conda install -y -q -c pytorch -c nvidia cudatoolkit\n",
        "%cd -q /content/GPT-SoVITS\n",
        "!conda install -y -q -c conda-forge gcc gxx ffmpeg cmake -c pytorch -c nvidia\n",
        "!/usr/local/bin/pip install -r requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏬ Downloading https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh...\n",
            "📦 Installing...\n",
            "📌 Adjusting configuration...\n",
            "🩹 Patching environment...\n",
            "⏲ Done in 0:00:21\n",
            "🔁 Restarting kernel...\n",
            "Cloning into 'GPT-SoVITS'...\n",
            "remote: Enumerating objects: 1542, done.\u001b[K\n",
            "remote: Counting objects: 100% (786/786), done.\u001b[K\n",
            "remote: Compressing objects: 100% (373/373), done.\u001b[K\n",
            "remote: Total 1542 (delta 528), reused 592 (delta 400), pack-reused 756\u001b[K\n",
            "Receiving objects: 100% (1542/1542), 2.84 MiB | 7.11 MiB/s, done.\n",
            "Resolving deltas: 100% (799/799), done.\n",
            "Channels:\n",
            " - pytorch\n",
            " - nvidia\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - cudatoolkit\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    conda-24.1.0               |   py39h06a4308_0         960 KB\n",
            "    cudatoolkit-11.7.0         |      hd8887f6_10       831.6 MB  nvidia\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       832.5 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  cudatoolkit        nvidia/linux-64::cudatoolkit-11.7.0-hd8887f6_10 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  conda                              23.11.0-py39h06a4308_0 --> 24.1.0-py39h06a4308_0 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
            "\n",
            "done\n",
            "Channels:\n",
            " - conda-forge\n",
            " - pytorch\n",
            " - nvidia\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - cmake\n",
            "    - ffmpeg\n",
            "    - gcc\n",
            "    - gxx\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _libgcc_mutex-0.1          |      conda_forge           3 KB  conda-forge\n",
            "    _openmp_mutex-4.5          |            2_gnu          23 KB  conda-forge\n",
            "    aom-3.8.1                  |       h59595ed_0         2.6 MB  conda-forge\n",
            "    binutils_impl_linux-64-2.40|       hf600244_0         5.2 MB  conda-forge\n",
            "    cairo-1.18.0               |       h3faef2a_0         959 KB  conda-forge\n",
            "    certifi-2023.11.17         |     pyhd8ed1ab_0         155 KB  conda-forge\n",
            "    cmake-3.28.2               |       hcfe8598_0        17.9 MB  conda-forge\n",
            "    dav1d-1.2.1                |       hd590300_0         742 KB  conda-forge\n",
            "    expat-2.5.0                |       hcb278e6_1         134 KB  conda-forge\n",
            "    ffmpeg-6.1.1               | gpl_h33edf06_103         9.3 MB  conda-forge\n",
            "    font-ttf-dejavu-sans-mono-2.37|       hab24e00_0         388 KB  conda-forge\n",
            "    font-ttf-inconsolata-3.000 |       h77eed37_0          94 KB  conda-forge\n",
            "    font-ttf-source-code-pro-2.038|       h77eed37_0         684 KB  conda-forge\n",
            "    font-ttf-ubuntu-0.83       |       h77eed37_1         1.5 MB  conda-forge\n",
            "    fontconfig-2.14.2          |       h14ed4e7_0         266 KB  conda-forge\n",
            "    fonts-conda-ecosystem-1    |                0           4 KB  conda-forge\n",
            "    fonts-conda-forge-1        |                0           4 KB  conda-forge\n",
            "    freetype-2.12.1            |       h267a509_2         620 KB  conda-forge\n",
            "    fribidi-1.0.10             |       h36c2ea0_0         112 KB  conda-forge\n",
            "    gcc-13.2.0                 |       h574f8da_2          26 KB  conda-forge\n",
            "    gcc_impl_linux-64-13.2.0   |       h338b0a0_5        50.8 MB  conda-forge\n",
            "    gettext-0.21.1             |       h27087fc_0         4.1 MB  conda-forge\n",
            "    gmp-6.3.0                  |       h59595ed_0         550 KB  conda-forge\n",
            "    gnutls-3.7.9               |       hb077bed_0         1.9 MB  conda-forge\n",
            "    graphite2-1.3.13           |    h58526e2_1001         102 KB  conda-forge\n",
            "    gxx-13.2.0                 |       h574f8da_2          26 KB  conda-forge\n",
            "    gxx_impl_linux-64-13.2.0   |       h338b0a0_5        13.0 MB  conda-forge\n",
            "    harfbuzz-8.3.0             |       h3d44ed6_0         1.5 MB  conda-forge\n",
            "    icu-73.2                   |       h59595ed_0        11.5 MB  conda-forge\n",
            "    kernel-headers_linux-64-2.6.32|      he073ed8_16         692 KB  conda-forge\n",
            "    lame-3.100                 |    h166bdaf_1003         496 KB  conda-forge\n",
            "    ld_impl_linux-64-2.40      |       h41732ed_0         688 KB  conda-forge\n",
            "    libabseil-20230802.1       | cxx17_h59595ed_0         1.2 MB  conda-forge\n",
            "    libarchive-3.6.2           |       h039dbb9_1         824 KB  conda-forge\n",
            "    libass-0.17.1              |       h8fe9dca_1         124 KB  conda-forge\n",
            "    libcurl-8.5.0              |       h251f7ec_0         416 KB\n",
            "    libdrm-2.4.114             |       h166bdaf_0         298 KB  conda-forge\n",
            "    libexpat-2.5.0             |       hcb278e6_1          76 KB  conda-forge\n",
            "    libgcc-devel_linux-64-13.2.0|     ha9c7c90_105         2.5 MB  conda-forge\n",
            "    libgcc-ng-13.2.0           |       h807b86a_5         752 KB  conda-forge\n",
            "    libglib-2.78.3             |       h783c2da_0         2.6 MB  conda-forge\n",
            "    libgomp-13.2.0             |       h807b86a_5         410 KB  conda-forge\n",
            "    libhwloc-2.9.3             |default_h554bfaf_1009         2.5 MB  conda-forge\n",
            "    libiconv-1.17              |       hd590300_2         689 KB  conda-forge\n",
            "    libidn2-2.3.7              |       hd590300_0         124 KB  conda-forge\n",
            "    libopenvino-2023.3.0       |       h2e90f83_0         5.7 MB  conda-forge\n",
            "    libopenvino-auto-batch-plugin-2023.3.0|       hd5fc58b_0         113 KB  conda-forge\n",
            "    libopenvino-auto-plugin-2023.3.0|       hd5fc58b_0         233 KB  conda-forge\n",
            "    libopenvino-hetero-plugin-2023.3.0|       h3ecfda7_0         178 KB  conda-forge\n",
            "    libopenvino-intel-cpu-plugin-2023.3.0|       h2e90f83_0         9.7 MB  conda-forge\n",
            "    libopenvino-intel-gpu-plugin-2023.3.0|       h2e90f83_0         7.8 MB  conda-forge\n",
            "    libopenvino-ir-frontend-2023.3.0|       h3ecfda7_0         194 KB  conda-forge\n",
            "    libopenvino-onnx-frontend-2023.3.0|       hfbc7f12_0         1.5 MB  conda-forge\n",
            "    libopenvino-paddle-frontend-2023.3.0|       hfbc7f12_0         644 KB  conda-forge\n",
            "    libopenvino-pytorch-frontend-2023.3.0|       h59595ed_0         938 KB  conda-forge\n",
            "    libopenvino-tensorflow-frontend-2023.3.0|       h0bff32c_0         1.1 MB  conda-forge\n",
            "    libopenvino-tensorflow-lite-frontend-2023.3.0|       h59595ed_0         446 KB  conda-forge\n",
            "    libopus-1.3.1              |       h7f98852_1         255 KB  conda-forge\n",
            "    libpciaccess-0.17          |       h166bdaf_0          39 KB  conda-forge\n",
            "    libpng-1.6.42              |       h2797004_0         282 KB  conda-forge\n",
            "    libprotobuf-4.25.1         |       hf27288f_0         2.6 MB  conda-forge\n",
            "    libsanitizer-13.2.0        |       h7e041cc_5         3.9 MB  conda-forge\n",
            "    libstdcxx-devel_linux-64-13.2.0|     ha9c7c90_105        12.4 MB  conda-forge\n",
            "    libstdcxx-ng-13.2.0        |       h7e041cc_5         3.7 MB  conda-forge\n",
            "    libtasn1-4.19.0            |       h166bdaf_0         114 KB  conda-forge\n",
            "    libunistring-0.9.10        |       h7f98852_0         1.4 MB  conda-forge\n",
            "    libuuid-2.38.1             |       h0b41bf4_0          33 KB  conda-forge\n",
            "    libuv-1.46.0               |       hd590300_0         872 KB  conda-forge\n",
            "    libva-2.20.0               |       hd590300_0         184 KB  conda-forge\n",
            "    libvpx-1.13.1              |       h59595ed_0         982 KB  conda-forge\n",
            "    libxcb-1.15                |       h0b41bf4_0         375 KB  conda-forge\n",
            "    libxml2-2.12.4             |       h232c23b_1         688 KB  conda-forge\n",
            "    libzlib-1.2.13             |       hd590300_5          60 KB  conda-forge\n",
            "    lzo-2.10                   |    h516909a_1000         314 KB  conda-forge\n",
            "    nettle-3.9.1               |       h7ab15ed_0         988 KB  conda-forge\n",
            "    ocl-icd-2.3.1              |       h7f98852_0         119 KB  conda-forge\n",
            "    ocl-icd-system-1.0.0       |                1           4 KB  conda-forge\n",
            "    openh264-2.4.0             |       h59595ed_0         719 KB  conda-forge\n",
            "    openssl-3.2.1              |       hd590300_0         2.7 MB  conda-forge\n",
            "    p11-kit-0.24.1             |       hc5aa10d_0         4.5 MB  conda-forge\n",
            "    pixman-0.43.2              |       h59595ed_0         378 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    h36c2ea0_1001           5 KB  conda-forge\n",
            "    pugixml-1.14               |       h59595ed_0         112 KB  conda-forge\n",
            "    rhash-1.4.4                |       hd590300_0         181 KB  conda-forge\n",
            "    snappy-1.1.10              |       h9fff704_0          38 KB  conda-forge\n",
            "    svt-av1-1.8.0              |       h59595ed_0         2.5 MB  conda-forge\n",
            "    sysroot_linux-64-2.12      |      he073ed8_16        14.6 MB  conda-forge\n",
            "    tbb-2021.11.0              |       h00ab1b0_1         191 KB  conda-forge\n",
            "    x264-1!164.3095            |       h166bdaf_2         877 KB  conda-forge\n",
            "    x265-3.5                   |       h924138e_3         3.2 MB  conda-forge\n",
            "    xorg-fixesproto-5.0        |    h7f98852_1002           9 KB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    h7f98852_1002          27 KB  conda-forge\n",
            "    xorg-libice-1.1.1          |       hd590300_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.4           |       h7391055_0          27 KB  conda-forge\n",
            "    xorg-libx11-1.8.7          |       h8ee46fc_0         809 KB  conda-forge\n",
            "    xorg-libxau-1.0.11         |       hd590300_0          14 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.3        |       h7f98852_0          19 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h0b41bf4_2          49 KB  conda-forge\n",
            "    xorg-libxfixes-5.0.3       |    h7f98852_1004          18 KB  conda-forge\n",
            "    xorg-libxrender-0.9.11     |       hd590300_0          37 KB  conda-forge\n",
            "    xorg-renderproto-0.11.1    |    h7f98852_1002           9 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    h0b41bf4_1003          30 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    h7f98852_1007          73 KB  conda-forge\n",
            "    zlib-1.2.13                |       hd590300_5          91 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       227.1 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  aom                conda-forge/linux-64::aom-3.8.1-h59595ed_0 \n",
            "  binutils_impl_lin~ conda-forge/linux-64::binutils_impl_linux-64-2.40-hf600244_0 \n",
            "  cairo              conda-forge/linux-64::cairo-1.18.0-h3faef2a_0 \n",
            "  cmake              conda-forge/linux-64::cmake-3.28.2-hcfe8598_0 \n",
            "  dav1d              conda-forge/linux-64::dav1d-1.2.1-hd590300_0 \n",
            "  expat              conda-forge/linux-64::expat-2.5.0-hcb278e6_1 \n",
            "  ffmpeg             conda-forge/linux-64::ffmpeg-6.1.1-gpl_h33edf06_103 \n",
            "  font-ttf-dejavu-s~ conda-forge/noarch::font-ttf-dejavu-sans-mono-2.37-hab24e00_0 \n",
            "  font-ttf-inconsol~ conda-forge/noarch::font-ttf-inconsolata-3.000-h77eed37_0 \n",
            "  font-ttf-source-c~ conda-forge/noarch::font-ttf-source-code-pro-2.038-h77eed37_0 \n",
            "  font-ttf-ubuntu    conda-forge/noarch::font-ttf-ubuntu-0.83-h77eed37_1 \n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.14.2-h14ed4e7_0 \n",
            "  fonts-conda-ecosy~ conda-forge/noarch::fonts-conda-ecosystem-1-0 \n",
            "  fonts-conda-forge  conda-forge/noarch::fonts-conda-forge-1-0 \n",
            "  freetype           conda-forge/linux-64::freetype-2.12.1-h267a509_2 \n",
            "  fribidi            conda-forge/linux-64::fribidi-1.0.10-h36c2ea0_0 \n",
            "  gcc                conda-forge/linux-64::gcc-13.2.0-h574f8da_2 \n",
            "  gcc_impl_linux-64  conda-forge/linux-64::gcc_impl_linux-64-13.2.0-h338b0a0_5 \n",
            "  gettext            conda-forge/linux-64::gettext-0.21.1-h27087fc_0 \n",
            "  gmp                conda-forge/linux-64::gmp-6.3.0-h59595ed_0 \n",
            "  gnutls             conda-forge/linux-64::gnutls-3.7.9-hb077bed_0 \n",
            "  graphite2          conda-forge/linux-64::graphite2-1.3.13-h58526e2_1001 \n",
            "  gxx                conda-forge/linux-64::gxx-13.2.0-h574f8da_2 \n",
            "  gxx_impl_linux-64  conda-forge/linux-64::gxx_impl_linux-64-13.2.0-h338b0a0_5 \n",
            "  harfbuzz           conda-forge/linux-64::harfbuzz-8.3.0-h3d44ed6_0 \n",
            "  kernel-headers_li~ conda-forge/noarch::kernel-headers_linux-64-2.6.32-he073ed8_16 \n",
            "  lame               conda-forge/linux-64::lame-3.100-h166bdaf_1003 \n",
            "  libabseil          conda-forge/linux-64::libabseil-20230802.1-cxx17_h59595ed_0 \n",
            "  libass             conda-forge/linux-64::libass-0.17.1-h8fe9dca_1 \n",
            "  libdrm             conda-forge/linux-64::libdrm-2.4.114-h166bdaf_0 \n",
            "  libexpat           conda-forge/linux-64::libexpat-2.5.0-hcb278e6_1 \n",
            "  libgcc-devel_linu~ conda-forge/noarch::libgcc-devel_linux-64-13.2.0-ha9c7c90_105 \n",
            "  libglib            conda-forge/linux-64::libglib-2.78.3-h783c2da_0 \n",
            "  libhwloc           conda-forge/linux-64::libhwloc-2.9.3-default_h554bfaf_1009 \n",
            "  libiconv           conda-forge/linux-64::libiconv-1.17-hd590300_2 \n",
            "  libidn2            conda-forge/linux-64::libidn2-2.3.7-hd590300_0 \n",
            "  libopenvino        conda-forge/linux-64::libopenvino-2023.3.0-h2e90f83_0 \n",
            "  libopenvino-auto-~ conda-forge/linux-64::libopenvino-auto-batch-plugin-2023.3.0-hd5fc58b_0 \n",
            "  libopenvino-auto-~ conda-forge/linux-64::libopenvino-auto-plugin-2023.3.0-hd5fc58b_0 \n",
            "  libopenvino-heter~ conda-forge/linux-64::libopenvino-hetero-plugin-2023.3.0-h3ecfda7_0 \n",
            "  libopenvino-intel~ conda-forge/linux-64::libopenvino-intel-cpu-plugin-2023.3.0-h2e90f83_0 \n",
            "  libopenvino-intel~ conda-forge/linux-64::libopenvino-intel-gpu-plugin-2023.3.0-h2e90f83_0 \n",
            "  libopenvino-ir-fr~ conda-forge/linux-64::libopenvino-ir-frontend-2023.3.0-h3ecfda7_0 \n",
            "  libopenvino-onnx-~ conda-forge/linux-64::libopenvino-onnx-frontend-2023.3.0-hfbc7f12_0 \n",
            "  libopenvino-paddl~ conda-forge/linux-64::libopenvino-paddle-frontend-2023.3.0-hfbc7f12_0 \n",
            "  libopenvino-pytor~ conda-forge/linux-64::libopenvino-pytorch-frontend-2023.3.0-h59595ed_0 \n",
            "  libopenvino-tenso~ conda-forge/linux-64::libopenvino-tensorflow-frontend-2023.3.0-h0bff32c_0 \n",
            "  libopenvino-tenso~ conda-forge/linux-64::libopenvino-tensorflow-lite-frontend-2023.3.0-h59595ed_0 \n",
            "  libopus            conda-forge/linux-64::libopus-1.3.1-h7f98852_1 \n",
            "  libpciaccess       conda-forge/linux-64::libpciaccess-0.17-h166bdaf_0 \n",
            "  libpng             conda-forge/linux-64::libpng-1.6.42-h2797004_0 \n",
            "  libprotobuf        conda-forge/linux-64::libprotobuf-4.25.1-hf27288f_0 \n",
            "  libsanitizer       conda-forge/linux-64::libsanitizer-13.2.0-h7e041cc_5 \n",
            "  libstdcxx-devel_l~ conda-forge/noarch::libstdcxx-devel_linux-64-13.2.0-ha9c7c90_105 \n",
            "  libtasn1           conda-forge/linux-64::libtasn1-4.19.0-h166bdaf_0 \n",
            "  libunistring       conda-forge/linux-64::libunistring-0.9.10-h7f98852_0 \n",
            "  libuuid            conda-forge/linux-64::libuuid-2.38.1-h0b41bf4_0 \n",
            "  libuv              conda-forge/linux-64::libuv-1.46.0-hd590300_0 \n",
            "  libva              conda-forge/linux-64::libva-2.20.0-hd590300_0 \n",
            "  libvpx             conda-forge/linux-64::libvpx-1.13.1-h59595ed_0 \n",
            "  libxcb             conda-forge/linux-64::libxcb-1.15-h0b41bf4_0 \n",
            "  libzlib            conda-forge/linux-64::libzlib-1.2.13-hd590300_5 \n",
            "  lzo                conda-forge/linux-64::lzo-2.10-h516909a_1000 \n",
            "  nettle             conda-forge/linux-64::nettle-3.9.1-h7ab15ed_0 \n",
            "  ocl-icd            conda-forge/linux-64::ocl-icd-2.3.1-h7f98852_0 \n",
            "  ocl-icd-system     conda-forge/linux-64::ocl-icd-system-1.0.0-1 \n",
            "  openh264           conda-forge/linux-64::openh264-2.4.0-h59595ed_0 \n",
            "  p11-kit            conda-forge/linux-64::p11-kit-0.24.1-hc5aa10d_0 \n",
            "  pixman             conda-forge/linux-64::pixman-0.43.2-h59595ed_0 \n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h36c2ea0_1001 \n",
            "  pugixml            conda-forge/linux-64::pugixml-1.14-h59595ed_0 \n",
            "  rhash              conda-forge/linux-64::rhash-1.4.4-hd590300_0 \n",
            "  snappy             conda-forge/linux-64::snappy-1.1.10-h9fff704_0 \n",
            "  svt-av1            conda-forge/linux-64::svt-av1-1.8.0-h59595ed_0 \n",
            "  sysroot_linux-64   conda-forge/noarch::sysroot_linux-64-2.12-he073ed8_16 \n",
            "  tbb                conda-forge/linux-64::tbb-2021.11.0-h00ab1b0_1 \n",
            "  x264               conda-forge/linux-64::x264-1!164.3095-h166bdaf_2 \n",
            "  x265               conda-forge/linux-64::x265-3.5-h924138e_3 \n",
            "  xorg-fixesproto    conda-forge/linux-64::xorg-fixesproto-5.0-h7f98852_1002 \n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h7f98852_1002 \n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.1.1-hd590300_0 \n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.4-h7391055_0 \n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.8.7-h8ee46fc_0 \n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.11-hd590300_0 \n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h7f98852_0 \n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h0b41bf4_2 \n",
            "  xorg-libxfixes     conda-forge/linux-64::xorg-libxfixes-5.0.3-h7f98852_1004 \n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.11-hd590300_0 \n",
            "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h7f98852_1002 \n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h0b41bf4_1003 \n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h7f98852_1007 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  icu                        pkgs/main::icu-73.1-h6a678d5_0 --> conda-forge::icu-73.2-h59595ed_0 \n",
            "  ld_impl_linux-64   pkgs/main::ld_impl_linux-64-2.38-h118~ --> conda-forge::ld_impl_linux-64-2.40-h41732ed_0 \n",
            "  libcurl                                  8.4.0-h251f7ec_1 --> 8.5.0-h251f7ec_0 \n",
            "  libgcc-ng          pkgs/main::libgcc-ng-11.2.0-h1234567_1 --> conda-forge::libgcc-ng-13.2.0-h807b86a_5 \n",
            "  libgomp              pkgs/main::libgomp-11.2.0-h1234567_1 --> conda-forge::libgomp-13.2.0-h807b86a_5 \n",
            "  libstdcxx-ng       pkgs/main::libstdcxx-ng-11.2.0-h12345~ --> conda-forge::libstdcxx-ng-13.2.0-h7e041cc_5 \n",
            "  libxml2              pkgs/main::libxml2-2.10.4-hf1b16e4_1 --> conda-forge::libxml2-2.12.4-h232c23b_1 \n",
            "  openssl              pkgs/main::openssl-3.0.12-h7f8727e_0 --> conda-forge::openssl-3.2.1-hd590300_0 \n",
            "  zlib                    pkgs/main::zlib-1.2.13-h5eee18b_0 --> conda-forge::zlib-1.2.13-hd590300_5 \n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  _libgcc_mutex           pkgs/main::_libgcc_mutex-0.1-main --> conda-forge::_libgcc_mutex-0.1-conda_forge \n",
            "  _openmp_mutex          pkgs/main::_openmp_mutex-5.1-1_gnu --> conda-forge::_openmp_mutex-4.5-2_gnu \n",
            "  certifi            pkgs/main/linux-64::certifi-2023.11.1~ --> conda-forge/noarch::certifi-2023.11.17-pyhd8ed1ab_0 \n",
            "  libarchive         pkgs/main::libarchive-3.6.2-h6ac8c49_2 --> conda-forge::libarchive-3.6.2-h039dbb9_1 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Collecting numpy (from -r requirements.txt (line 1))\n",
            "  Downloading numpy-1.26.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy (from -r requirements.txt (line 2))\n",
            "  Downloading scipy-1.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard (from -r requirements.txt (line 3))\n",
            "  Downloading tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting librosa==0.9.2 (from -r requirements.txt (line 4))\n",
            "  Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numba==0.56.4 (from -r requirements.txt (line 5))\n",
            "  Downloading numba-0.56.4-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning (from -r requirements.txt (line 6))\n",
            "  Downloading pytorch_lightning-2.1.4-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting gradio==3.38.0 (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-3.38.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting ffmpeg-python (from -r requirements.txt (line 8))\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Collecting onnxruntime (from -r requirements.txt (line 9))\n",
            "  Downloading onnxruntime-1.17.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (4.65.0)\n",
            "Collecting funasr>=1.0.0 (from -r requirements.txt (line 11))\n",
            "  Downloading funasr-1.0.5-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting cn2an (from -r requirements.txt (line 12))\n",
            "  Downloading cn2an-0.5.22-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pypinyin (from -r requirements.txt (line 13))\n",
            "  Downloading pypinyin-0.50.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pyopenjtalk (from -r requirements.txt (line 14))\n",
            "  Downloading pyopenjtalk-0.3.3.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting g2p_en (from -r requirements.txt (line 15))\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio (from -r requirements.txt (line 16))\n",
            "  Downloading torchaudio-2.2.0-cp39-cp39-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting modelscope==1.10.0 (from -r requirements.txt (line 17))\n",
            "  Downloading modelscope-1.10.0-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting sentencepiece (from -r requirements.txt (line 18))\n",
            "  Downloading sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers (from -r requirements.txt (line 19))\n",
            "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet (from -r requirements.txt (line 20))\n",
            "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting PyYAML (from -r requirements.txt (line 21))\n",
            "  Downloading PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting psutil (from -r requirements.txt (line 22))\n",
            "  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting jieba_fast (from -r requirements.txt (line 23))\n",
            "  Downloading jieba_fast-0.53.tar.gz (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba (from -r requirements.txt (line 24))\n",
            "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting audioread>=2.1.9 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting scikit-learn>=0.19.1 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading scikit_learn-1.4.0-1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting joblib>=0.14 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting decorator>=4.0.10 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting resampy>=0.2.2 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting soundfile>=0.10.2 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pooch>=1.0 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading pooch-1.8.0-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (23.1)\n",
            "Collecting llvmlite<0.40,>=0.39.0dev0 (from numba==0.56.4->-r requirements.txt (line 5))\n",
            "  Downloading llvmlite-0.39.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy (from -r requirements.txt (line 1))\n",
            "  Downloading numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from numba==0.56.4->-r requirements.txt (line 5)) (68.2.2)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting aiohttp~=3.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting altair<6.0,>=4.2.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting fastapi (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading fastapi-0.109.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting ffmpy (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.2.10 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.8.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting huggingface-hub>=0.14.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting jinja2<4.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting markdown-it-py>=2.0.0 (from markdown-it-py[linkify]>=2.0.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting markupsafe~=2.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading MarkupSafe-2.1.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting matplotlib~=3.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading matplotlib-3.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson~=3.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading orjson-3.9.12-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas<3.0,>=1.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pandas-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting pillow<11.0,>=8.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pillow-10.2.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pydantic-2.6.0-py3-none-any.whl.metadata (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests~=2.0 in /usr/local/lib/python3.9/site-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (2.31.0)\n",
            "Collecting semantic-version~=2.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting typing-extensions~=4.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading uvicorn-0.27.0.post1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting addict (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting attrs (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting datasets>=2.14.5 (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting einops (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting filelock>=3.3.0 (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting gast>=0.2.2 (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
            "Collecting oss2 (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading oss2-2.18.4.tar.gz (278 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyarrow!=9.0.0,>=6.0.0 (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading pyarrow-15.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting python-dateutil>=2.1 (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simplejson>=3.3.0 (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading simplejson-3.19.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n",
            "Collecting sortedcontainers>=1.5.9 (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 17)) (1.26.18)\n",
            "Collecting yapf (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading yapf-0.40.2-py3-none-any.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading grpcio-1.60.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting google-auth<3,>=1.6.3 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading google_auth-2.27.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading Markdown-3.5.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting protobuf<4.24,>=3.19.6 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
            "Collecting six>1.9 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting torch>=1.12.0 (from pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading torch-2.2.0-cp39-cp39-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting fsspec>=2022.5.0 (from fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting future (from ffmpeg-python->-r requirements.txt (line 8))\n",
            "  Downloading future-0.18.3.tar.gz (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting coloredlogs (from onnxruntime->-r requirements.txt (line 9))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flatbuffers (from onnxruntime->-r requirements.txt (line 9))\n",
            "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Collecting sympy (from onnxruntime->-r requirements.txt (line 9))\n",
            "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jamo (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Collecting kaldiio>=2.17.0 (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading kaldiio-2.18.0-py3-none-any.whl (28 kB)\n",
            "Collecting torch-complex (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading torch_complex-0.4.3-py3-none-any.whl (9.1 kB)\n",
            "Collecting pytorch-wpe (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading pytorch_wpe-0.0.1-py3-none-any.whl (8.1 kB)\n",
            "Collecting editdistance>=0.5.2 (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading editdistance-0.6.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.4/282.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hdbscan (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting umap-learn (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading umap-learn-0.5.5.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jaconv (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading jaconv-0.3.4.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hydra-core>=1.3.2 (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting proces>=0.1.3 (from cn2an->-r requirements.txt (line 12))\n",
            "  Downloading proces-0.1.7-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting nltk>=3.2.4 (from g2p_en->-r requirements.txt (line 15))\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting inflect>=0.3.1 (from g2p_en->-r requirements.txt (line 15))\n",
            "  Downloading inflect-7.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting distance>=0.1.3 (from g2p_en->-r requirements.txt (line 15))\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting networkx (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2.0 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading triton-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 19))\n",
            "  Downloading regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.19,>=0.14 (from transformers->-r requirements.txt (line 19))\n",
            "  Downloading tokenizers-0.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.4.1 (from transformers->-r requirements.txt (line 19))\n",
            "  Downloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting async-timeout<5.0,>=4.0 (from aiohttp~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting jsonschema>=3.0 (from altair<6.0,>=4.2.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting toolz (from altair<6.0,>=4.2.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting xxhash (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2022.5.0 (from fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.3.2->funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.3.2->funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting importlib-metadata>=4.4 (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading importlib_metadata-7.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.0.0->markdown-it-py[linkify]>=2.0.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading contourpy-1.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading fonttools-4.47.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (157 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.6/157.6 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting importlib-resources>=3.2.0 (from matplotlib~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is still looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click (from nltk>=3.2.4->g2p_en->-r requirements.txt (line 15))\n",
            "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pytz>=2020.1 (from pandas<3.0,>=1.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas<3.0,>=1.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading tzdata-2023.4-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.9/site-packages (from pooch>=1.0->librosa==0.9.2->-r requirements.txt (line 4)) (3.10.0)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pydantic-core==2.16.1 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pydantic_core-2.16.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests~=2.0->gradio==3.38.0->-r requirements.txt (line 7)) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests~=2.0->gradio==3.38.0->-r requirements.txt (line 7)) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests~=2.0->gradio==3.38.0->-r requirements.txt (line 7)) (2023.11.17)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.19.1->librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/site-packages (from soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 4)) (1.16.0)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->-r requirements.txt (line 9))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.36.0,>=0.35.0 (from fastapi->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading starlette-0.35.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting cython<3,>=0.27 (from hdbscan->funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Using cached Cython-0.29.37-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (3.1 kB)\n",
            "Collecting anyio (from httpx->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting httpcore==1.* (from httpx->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting sniffio (from httpx->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting crcmod>=1.7 (from oss2->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycryptodome>=3.4.7 (from oss2->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting aliyun-python-sdk-kms>=2.4.1 (from oss2->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading aliyun_python_sdk_kms-2.16.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting aliyun-python-sdk-core>=2.13.12 (from oss2->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading aliyun-python-sdk-core-2.14.0.tar.gz (443 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.0/443.0 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mpmath>=0.19 (from sympy->onnxruntime->-r requirements.txt (line 9))\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pynndescent>=0.5 (from umap-learn->funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading pynndescent-0.5.11-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting tomli>=2.0.1 (from yapf->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cryptography>=2.6.0 in /usr/local/lib/python3.9/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2->modelscope==1.10.0->-r requirements.txt (line 17)) (41.0.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 4)) (2.21)\n",
            "Collecting zipp>=0.5 (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading referencing-0.33.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading rpds_py-0.17.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting exceptiongroup>=1.0.2 (from anyio->httpx->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading multiprocess-0.70.15-py39-none-any.whl.metadata (7.2 kB)\n",
            "Downloading gradio-3.38.0-py3-none-any.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading modelscope-1.10.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.5/38.5 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.1.4-py3-none-any.whl (778 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.1/778.1 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.17.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funasr-1.0.5-py3-none-any.whl (549 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.6/549.6 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cn2an-0.5.22-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypinyin-0.50.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.2.0-cp39-cp39-manylinux1_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.0-cp39-cp39-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m738.9/738.9 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.9/996.9 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
            "Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
            "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth-2.27.0-py2.py3-none-any.whl (186 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.8/186.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading gradio_client-0.8.1-py3-none-any.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.60.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inflect-7.0.0-py3-none-any.whl (34 kB)\n",
            "Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.9/103.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading matplotlib-3.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.9.12-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.2.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pooch-1.8.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading proces-0.1.7-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-15.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.6.0-py3-none-any.whl (394 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.16.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.4/773.4 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.4.0-1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simplejson-3.19.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Downloading uvicorn-0.27.0.post1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
            "Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yapf-0.40.2-py3-none-any.whl (254 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aliyun_python_sdk_kms-2.16.2-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
            "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.0/311.0 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached Cython-0.29.37-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fonttools-4.47.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\n",
            "Downloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
            "Downloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.11-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.35.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyio-4.2.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Downloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.3/304.3 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.8/193.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
            "Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading referencing-0.33.0-py3-none-any.whl (26 kB)\n",
            "Downloading rpds_py-0.17.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyopenjtalk, jieba_fast, jieba, distance, antlr4-python3-runtime, ffmpy, future, hdbscan, jaconv, oss2, umap-learn, aliyun-python-sdk-core, crcmod\n",
            "  Building wheel for pyopenjtalk (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyopenjtalk: filename=pyopenjtalk-0.3.3-cp39-cp39-linux_x86_64.whl size=1195586 sha256=d65657cc0976de6c4b1181829e303cc5135fe0942fbf2afafcc9de188ba788fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/1e/46/f15c401c93d6fdf1dba5c9268033c057d033f6243e8f51e806\n",
            "  Building wheel for jieba_fast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba_fast: filename=jieba_fast-0.53-cp39-cp39-linux_x86_64.whl size=7629272 sha256=2d4e1db1dde9a9aeb83251a6fc1a22be35f4a9b270c7bae04ca486b67106c5b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/04/c8/5c563e7f58588aadfa5af1353a086ef467eb1dadd2fcfac622\n",
            "  Building wheel for jieba (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=920d71a5efe75265393807167eff760b7fcd46c49bc799da0754bb577da931fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16258 sha256=e2f61524e9ba1d3f671cf824942a5552ac23120ddbf930fd9a8027dda66f1a99\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/b3/aa/04241cced6d1722b132273b1d6aafba317887ec004f48b853a\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=6943720ddb93aaedb6bb1dacd2d36676c646b4735264fc90bf06aa2e482c61b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/cf/80/f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=61f06fba8cd798d5329ef209a43bd6b71157ae5189b0a92ed10287b57ab72144\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/f1/8d/367922b023b526b7c2ced5db30932def7b18cf39d7ac6e8572\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492024 sha256=c8b202edbe1336a0b832bda9e5677a2e248013196379ae760008172aca359e06\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/5d/6a/2e53874f7ec4e2bede522385439531fafec8fafe005b5c3d1b\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.33-cp39-cp39-linux_x86_64.whl size=737965 sha256=e83dd5be4fbf2a8dfaa0ad013ef0269a6624fbbee711051a6bdd74ead0cb13e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/5e/ed/5989da4cc423a222a47cbb4fde5d6c0eff4590d922e45f233c\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jaconv: filename=jaconv-0.3.4-py3-none-any.whl size=16416 sha256=185e8ec813bad32e9955c7a16fee71ab95ae550db7284b8271c63c6d61aab6e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/de/f1/55f605920db3666d30215331bc85f24686dde9b95b473ae41b\n",
            "  Building wheel for oss2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oss2: filename=oss2-2.18.4-py3-none-any.whl size=115939 sha256=cf521f9993935f83d470e31043e0002af9df1c49b2e3dd83adf330dbfc6ef650\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/65/8e/0417b076271c1d16cd41db00820b121cdc3fab91e2fbeb2aae\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.5-py3-none-any.whl size=86832 sha256=29cd94d2c001b023991f99d7c37123b1ab253e26bda574334b0515329410007a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/01/17/26951217a11fb724b7027c6dd5b620b6d368104f7e6d4171fc\n",
            "  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.14.0-py3-none-any.whl size=535290 sha256=ffde844e2ad129a4079a0952dc04965803fa435f6040708955fe80ee4b01ccb1\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/bd/a9/ef224c78c3aa426c4867927f46ebb6fd1fd705c6c4bafde111\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp39-cp39-linux_x86_64.whl size=23291 sha256=e80de4c81c8ee9b6d65042ed6b445439482fd5fed2211125da6565df83d53386\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/6c/a6/ffdd136310039bf226f2707a9a8e6857be7d70a3fc061f6b36\n",
            "Successfully built pyopenjtalk jieba_fast jieba distance antlr4-python3-runtime ffmpy future hdbscan jaconv oss2 umap-learn aliyun-python-sdk-core crcmod\n",
            "Installing collected packages: sortedcontainers, sentencepiece, pytz, pydub, mpmath, jieba_fast, jieba, jamo, jaconv, flatbuffers, ffmpy, distance, crcmod, antlr4-python3-runtime, addict, zipp, xxhash, websockets, uc-micro-py, tzdata, typing-extensions, toolz, tomli, threadpoolctl, tensorboard-data-server, sympy, sniffio, six, simplejson, semantic-version, safetensors, rpds-py, regex, PyYAML, python-multipart, pypinyin, pyparsing, pycryptodome, pyasn1, pyarrow-hotfix, psutil, protobuf, proces, pillow, orjson, oauthlib, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, mdurl, markupsafe, llvmlite, kiwisolver, joblib, jmespath, humanfriendly, h11, grpcio, gast, future, fsspec, frozenlist, fonttools, filelock, exceptiongroup, einops, editdistance, dill, decorator, cython, cycler, click, chardet, cachetools, audioread, attrs, async-timeout, annotated-types, aiofiles, absl-py, yarl, werkzeug, uvicorn, triton, torch-complex, soundfile, scipy, rsa, requests-oauthlib, referencing, pytorch-wpe, python-dateutil, pyopenjtalk, pydantic-core, pyasn1-modules, pyarrow, pooch, omegaconf, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, nltk, multiprocess, markdown-it-py, linkify-it-py, lightning-utilities, kaldiio, jinja2, importlib-resources, importlib-metadata, huggingface-hub, httpcore, ffmpeg-python, contourpy, coloredlogs, cn2an, anyio, aiosignal, yapf, tokenizers, starlette, scikit-learn, resampy, pydantic, pandas, onnxruntime, nvidia-cusolver-cu12, mdit-py-plugins, matplotlib, markdown, jsonschema-specifications, hydra-core, httpx, google-auth, aliyun-python-sdk-core, aiohttp, transformers, torch, pynndescent, librosa, jsonschema, inflect, hdbscan, gradio-client, google-auth-oauthlib, fastapi, aliyun-python-sdk-kms, umap-learn, torchmetrics, torchaudio, tensorboard, oss2, g2p_en, datasets, altair, pytorch-lightning, modelscope, gradio, funasr\n",
            "Successfully installed PyYAML-6.0.1 absl-py-2.1.0 addict-2.4.0 aiofiles-23.2.1 aiohttp-3.9.3 aiosignal-1.3.1 aliyun-python-sdk-core-2.14.0 aliyun-python-sdk-kms-2.16.2 altair-5.2.0 annotated-types-0.6.0 antlr4-python3-runtime-4.9.3 anyio-4.2.0 async-timeout-4.0.3 attrs-23.2.0 audioread-3.0.1 cachetools-5.3.2 chardet-5.2.0 click-8.1.7 cn2an-0.5.22 coloredlogs-15.0.1 contourpy-1.2.0 crcmod-1.7 cycler-0.12.1 cython-0.29.37 datasets-2.16.1 decorator-5.1.1 dill-0.3.7 distance-0.1.3 editdistance-0.6.2 einops-0.7.0 exceptiongroup-1.2.0 fastapi-0.109.0 ffmpeg-python-0.2.0 ffmpy-0.3.1 filelock-3.13.1 flatbuffers-23.5.26 fonttools-4.47.2 frozenlist-1.4.1 fsspec-2023.10.0 funasr-1.0.5 future-0.18.3 g2p_en-2.1.0 gast-0.5.4 google-auth-2.27.0 google-auth-oauthlib-1.2.0 gradio-3.38.0 gradio-client-0.8.1 grpcio-1.60.1 h11-0.14.0 hdbscan-0.8.33 httpcore-1.0.2 httpx-0.26.0 huggingface-hub-0.20.3 humanfriendly-10.0 hydra-core-1.3.2 importlib-metadata-7.0.1 importlib-resources-6.1.1 inflect-7.0.0 jaconv-0.3.4 jamo-0.4.1 jieba-0.42.1 jieba_fast-0.53 jinja2-3.1.3 jmespath-0.10.0 joblib-1.3.2 jsonschema-4.21.1 jsonschema-specifications-2023.12.1 kaldiio-2.18.0 kiwisolver-1.4.5 librosa-0.9.2 lightning-utilities-0.10.1 linkify-it-py-2.0.2 llvmlite-0.39.1 markdown-3.5.2 markdown-it-py-2.2.0 markupsafe-2.1.4 matplotlib-3.8.2 mdit-py-plugins-0.3.3 mdurl-0.1.2 modelscope-1.10.0 mpmath-1.3.0 multidict-6.0.5 multiprocess-0.70.15 networkx-3.2.1 nltk-3.8.1 numba-0.56.4 numpy-1.23.5 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 oauthlib-3.2.2 omegaconf-2.3.0 onnxruntime-1.17.0 orjson-3.9.12 oss2-2.18.4 pandas-2.2.0 pillow-10.2.0 pooch-1.8.0 proces-0.1.7 protobuf-4.23.4 psutil-5.9.8 pyarrow-15.0.0 pyarrow-hotfix-0.6 pyasn1-0.5.1 pyasn1-modules-0.3.0 pycryptodome-3.20.0 pydantic-2.6.0 pydantic-core-2.16.1 pydub-0.25.1 pynndescent-0.5.11 pyopenjtalk-0.3.3 pyparsing-3.1.1 pypinyin-0.50.0 python-dateutil-2.8.2 python-multipart-0.0.6 pytorch-lightning-2.1.4 pytorch-wpe-0.0.1 pytz-2024.1 referencing-0.33.0 regex-2023.12.25 requests-oauthlib-1.3.1 resampy-0.4.2 rpds-py-0.17.1 rsa-4.9 safetensors-0.4.2 scikit-learn-1.4.0 scipy-1.12.0 semantic-version-2.10.0 sentencepiece-0.1.99 simplejson-3.19.2 six-1.16.0 sniffio-1.3.0 sortedcontainers-2.4.0 soundfile-0.12.1 starlette-0.35.1 sympy-1.12 tensorboard-2.15.1 tensorboard-data-server-0.7.2 threadpoolctl-3.2.0 tokenizers-0.15.1 tomli-2.0.1 toolz-0.12.1 torch-2.2.0 torch-complex-0.4.3 torchaudio-2.2.0 torchmetrics-1.3.0.post0 transformers-4.37.2 triton-2.2.0 typing-extensions-4.9.0 tzdata-2023.4 uc-micro-py-1.0.2 umap-learn-0.5.5 uvicorn-0.27.0.post1 websockets-11.0.3 werkzeug-3.0.1 xxhash-3.4.1 yapf-0.40.2 yarl-1.9.4 zipp-3.17.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download pretrained models 下载预训练模型\n",
        "!mkdir -p /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
        "!mkdir -p /content/GPT-SoVITS/tools/damo_asr/models\n",
        "!mkdir -p /content/GPT-SoVITS/tools/uvr5\n",
        "%cd /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
        "!git clone https://huggingface.co/lj1995/GPT-SoVITS\n",
        "%cd /content/GPT-SoVITS/tools/damo_asr/models\n",
        "!git clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git\n",
        "# @title UVR5 pretrains 安装uvr5模型\n",
        "%cd /content/GPT-SoVITS/tools/uvr5\n",
        "!git clone https://huggingface.co/Delik/uvr5_weights\n",
        "!git config core.sparseCheckout true\n",
        "!mv /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/* /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/"
      ],
      "metadata": {
        "id": "0NgxXg5sjv7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d88001ac-3820-44d1-bd8a-2ffe8e2f3d8e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "/content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Cloning into 'GPT-SoVITS'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 20 (delta 1), reused 0 (delta 0), pack-reused 4\u001b[K\n",
            "Unpacking objects: 100% (20/20), 102.34 KiB | 1.68 MiB/s, done.\n",
            "Filtering content: 100% (5/5), 1.11 GiB | 70.03 MiB/s, done.\n",
            "/content/GPT-SoVITS/tools/damo_asr/models\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Cloning into 'speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch'...\n",
            "remote: Enumerating objects: 465, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 465 (delta 38), reused 57 (delta 30), pack-reused 398\u001b[K\n",
            "Receiving objects: 100% (465/465), 1.12 GiB | 22.25 MiB/s, done.\n",
            "Resolving deltas: 100% (268/268), done.\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Cloning into 'speech_fsmn_vad_zh-cn-16k-common-pytorch'...\n",
            "remote: Enumerating objects: 180, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 180 (delta 16), reused 23 (delta 10), pack-reused 145\u001b[K\n",
            "Receiving objects: 100% (180/180), 4.66 MiB | 19.55 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Cloning into 'punc_ct-transformer_zh-cn-common-vocab272727-pytorch'...\n",
            "remote: Enumerating objects: 167, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 167 (delta 23), reused 30 (delta 14), pack-reused 120\u001b[K\n",
            "Receiving objects: 100% (167/167), 257.56 MiB | 31.94 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n",
            "/content/GPT-SoVITS/tools/uvr5\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Cloning into 'uvr5_weights'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 15 (delta 1), reused 0 (delta 0), pack-reused 3\u001b[K\n",
            "Unpacking objects: 100% (15/15), 3.25 KiB | 1.08 MiB/s, done.\n",
            "Filtering content: 100% (9/9), 594.44 MiB | 88.77 MiB/s, done.\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title launch WebUI 启动WebUI\n",
        "!/usr/local/bin/pip install ipykernel\n",
        "!sed -i '9s/False/True/' /content/GPT-SoVITS/config.py\n",
        "%cd /content/GPT-SoVITS/\n",
        "!/usr/local/bin/python  webui.py"
      ],
      "metadata": {
        "id": "4oRGUzkrk8C7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3709e2c-9806-49e4-a3d0-7a914d58ca1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Collecting ipykernel\n",
            "  Downloading ipykernel-6.29.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting comm>=0.1.1 (from ipykernel)\n",
            "  Downloading comm-0.2.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting debugpy>=1.6.5 (from ipykernel)\n",
            "  Downloading debugpy-1.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting ipython>=7.23.1 (from ipykernel)\n",
            "  Downloading ipython-8.18.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel)\n",
            "  Downloading jupyter_client-8.6.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel)\n",
            "  Downloading jupyter_core-5.7.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting matplotlib-inline>=0.1 (from ipykernel)\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Collecting nest-asyncio (from ipykernel)\n",
            "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from ipykernel) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/site-packages (from ipykernel) (5.9.8)\n",
            "Collecting pyzmq>=24 (from ipykernel)\n",
            "  Downloading pyzmq-25.1.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting tornado>=6.1 (from ipykernel)\n",
            "  Downloading tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting traitlets>=5.4.0 (from ipykernel)\n",
            "  Downloading traitlets-5.14.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting prompt-toolkit<3.1.0,>=3.0.41 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading prompt_toolkit-3.0.43-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pygments>=2.4.0 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading pygments-2.17.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting stack-data (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (1.2.0)\n",
            "Collecting pexpect>4.3 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (7.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (3.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel) (3.17.0)\n",
            "Collecting parso<0.9.0,>=0.8.3 (from jedi>=0.16->ipython>=7.23.1->ipykernel)\n",
            "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=7.23.1->ipykernel)\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting wcwidth (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel)\n",
            "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
            "Collecting executing>=1.2.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading executing-2.0.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting pure-eval (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Downloading ipykernel-6.29.0-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.1/116.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.1-py3-none-any.whl (7.2 kB)\n",
            "Downloading debugpy-1.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipython-8.18.1-py3-none-any.whl (808 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.2/808.2 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-8.6.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_core-5.7.1-py3-none-any.whl (28 kB)\n",
            "Downloading pyzmq-25.1.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.4/435.4 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading traitlets-5.14.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
            "Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prompt_toolkit-3.0.43-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.1/386.1 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading executing-2.0.1-py2.py3-none-any.whl (24 kB)\n",
            "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
            "Installing collected packages: wcwidth, pure-eval, ptyprocess, traitlets, tornado, pyzmq, pygments, prompt-toolkit, pexpect, parso, nest-asyncio, executing, debugpy, asttokens, stack-data, matplotlib-inline, jupyter-core, jedi, comm, jupyter-client, ipython, ipykernel\n",
            "Successfully installed asttokens-2.4.1 comm-0.2.1 debugpy-1.8.0 executing-2.0.1 ipykernel-6.29.0 ipython-8.18.1 jedi-0.19.1 jupyter-client-8.6.0 jupyter-core-5.7.1 matplotlib-inline-0.1.6 nest-asyncio-1.6.0 parso-0.8.3 pexpect-4.9.0 prompt-toolkit-3.0.43 ptyprocess-0.7.0 pure-eval-0.2.2 pygments-2.17.2 pyzmq-25.1.2 stack-data-0.6.3 tornado-6.4 traitlets-5.14.1 wcwidth-0.2.13\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "/content/GPT-SoVITS\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Running on local URL:  http://0.0.0.0:9874\n",
            "Running on public URL: https://cf9efab43bdabb6ca0.gradio.live\n",
            "\"/usr/local/bin/python\" tools/uvr5/webui.py \"cuda\" True 9873 True\n",
            "Running on local URL:  http://0.0.0.0:9873\n",
            "Running on public URL: https://99d68eaacd755de3b1.gradio.live\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GPT-SoVITS/tools/uvr5/webui.py\", line 113, in uvr\n",
            "    del pre_fun.model\n",
            "UnboundLocalError: local variable 'pre_fun' referenced before assignment\n",
            "/content/GPT-SoVITS/tools/uvr5/webui.py:78: FutureWarning: Pass orig_sr=24000, target_sr=44100 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  y_resampled = librosa.resample(y, sr, 44100)\n",
            "/content/GPT-SoVITS/tools/uvr5/vr.py:62: FutureWarning: Pass sr=44100, mono=False as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  ) = librosa.core.load(  # 理论上librosa读取可能对某些音频有bug，应该上ffmpeg读取，但是太麻烦了弃坑\n",
            "/content/GPT-SoVITS/tools/uvr5/lib/lib_v5/spec_utils.py:81: FutureWarning: Pass n_fft=960 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  spec_right = librosa.stft(wave_right, n_fft, hop_length=hop_length)\n",
            "/content/GPT-SoVITS/tools/uvr5/vr.py:72: FutureWarning: Pass orig_sr=44100, target_sr=14700 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  X_wave[d] = librosa.core.resample(\n",
            "/content/GPT-SoVITS/tools/uvr5/lib/lib_v5/spec_utils.py:81: FutureWarning: Pass n_fft=512 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  spec_right = librosa.stft(wave_right, n_fft, hop_length=hop_length)\n",
            "/content/GPT-SoVITS/tools/uvr5/vr.py:72: FutureWarning: Pass orig_sr=14700, target_sr=7350 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  X_wave[d] = librosa.core.resample(\n",
            "/content/GPT-SoVITS/tools/uvr5/lib/lib_v5/spec_utils.py:81: FutureWarning: Pass n_fft=320 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  spec_right = librosa.stft(wave_right, n_fft, hop_length=hop_length)\n",
            "/content/GPT-SoVITS/tools/uvr5/vr.py:72: FutureWarning: Pass orig_sr=7350, target_sr=7350 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  X_wave[d] = librosa.core.resample(\n",
            "/content/GPT-SoVITS/tools/uvr5/lib/lib_v5/spec_utils.py:81: FutureWarning: Pass n_fft=640 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  spec_right = librosa.stft(wave_right, n_fft, hop_length=hop_length)\n",
            "100% 19/19 [00:02<00:00,  7.37it/s]\n",
            "/content/GPT-SoVITS/tools/uvr5/lib/lib_v5/spec_utils.py:396: FutureWarning: Pass orig_sr=7350, target_sr=7350 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  wave = librosa.resample(\n",
            "/content/GPT-SoVITS/tools/uvr5/lib/lib_v5/spec_utils.py:422: FutureWarning: Pass orig_sr=7350, target_sr=14700 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  wave = librosa.core.resample(wave2, bp[\"sr\"], sr, res_type=\"scipy\")\n",
            "/content/GPT-SoVITS/tools/uvr5/lib/lib_v5/spec_utils.py:422: FutureWarning: Pass orig_sr=14700, target_sr=44100 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  wave = librosa.core.resample(wave2, bp[\"sr\"], sr, res_type=\"scipy\")\n",
            "\"/usr/local/bin/python\" tools/slice_audio.py \"/content/GPT-SoVITS/3.flac\" \"output/slicer_opt\" -34 4000 300 10 500 0.9 0.25 0 1\n",
            "执行完毕，请检查输出文件\n",
            "\"/usr/local/bin/python\" tools/damo_asr/cmd-asr.py \"/content/GPT-SoVITS/output/slicer_opt\"\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "rtf_avg: 0.093: 100% 1/1 [00:00<00:00,  2.92it/s]\n",
            "time cost vad: 0.344\n",
            "rtf_avg: 0.310, time_speech:  3.680, time_escape: 1.140: 100% 1/1 [00:01<00:00,  1.15s/it]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 11.89it/s]\n",
            "time cost vad: 0.084\n",
            "rtf_avg: 0.017, time_speech:  6.510, time_escape: 0.113: 100% 1/1 [00:00<00:00,  7.87it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 14.89it/s]\n",
            "time cost vad: 0.067\n",
            "rtf_avg: 0.019, time_speech:  5.080, time_escape: 0.099: 100% 1/1 [00:00<00:00,  8.95it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 15.28it/s]\n",
            "time cost vad: 0.066\n",
            "rtf_avg: 0.023, time_speech:  4.730, time_escape: 0.107: 100% 1/1 [00:00<00:00,  8.26it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00,  9.62it/s]\n",
            "time cost vad: 0.104\n",
            "rtf_avg: 0.018, time_speech:  7.660, time_escape: 0.136: 100% 1/1 [00:00<00:00,  6.51it/s]\n",
            "rtf_avg: 0.016: 100% 1/1 [00:00<00:00, 18.68it/s]\n",
            "time cost vad: 0.054\n",
            "rtf_avg: 0.033, time_speech:  3.250, time_escape: 0.106: 100% 1/1 [00:00<00:00,  8.39it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 16.69it/s]\n",
            "time cost vad: 0.060\n",
            "rtf_avg: 0.026, time_speech:  4.170, time_escape: 0.107: 100% 1/1 [00:00<00:00,  8.35it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 17.75it/s]\n",
            "time cost vad: 0.057\n",
            "rtf_avg: 0.036, time_speech:  4.270, time_escape: 0.154: 100% 1/1 [00:00<00:00,  5.97it/s]\n",
            "rtf_avg: 0.021: 100% 1/1 [00:00<00:00, 10.92it/s]\n",
            "time cost vad: 0.092\n",
            "rtf_avg: 0.029, time_speech:  4.250, time_escape: 0.124: 100% 1/1 [00:00<00:00,  6.89it/s]\n",
            "rtf_avg: 0.022: 100% 1/1 [00:00<00:00, 10.34it/s]\n",
            "time cost vad: 0.097\n",
            "rtf_avg: 0.030, time_speech:  4.350, time_escape: 0.131: 100% 1/1 [00:00<00:00,  6.36it/s]\n",
            "\"/usr/local/bin/python\" tools/subfix_webui.py --load_list \"/content/GPT-SoVITS/output/asr_opt/slicer_opt.list\" --webui_port 9871 --is_share True\n",
            "/content/GPT-SoVITS/tools/subfix_webui.py:366: GradioUnusedKwargWarning: You have unused kwarg parameters in Button, please remove them: {'link': '?__theme=light'}\n",
            "  btn_theme_dark = gr.Button(\"Light Theme\", link=\"?__theme=light\", scale=1)\n",
            "/content/GPT-SoVITS/tools/subfix_webui.py:367: GradioUnusedKwargWarning: You have unused kwarg parameters in Button, please remove them: {'link': '?__theme=dark'}\n",
            "  btn_theme_light = gr.Button(\"Dark Theme\", link=\"?__theme=dark\", scale=1)\n",
            "Running on local URL:  http://0.0.0.0:9871\n",
            "Running on public URL: https://ef104b7d631af86004.gradio.live\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "Building prefix dict from the default dictionary ...\n",
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Dumping model to file cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Loading model cost 1.418 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "Loading model cost 1.461 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py\n",
            "Some weights of the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/3-get-semantic.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/3-get-semantic.py\n",
            "/usr/local/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "/usr/local/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/s2_train.py --config \"/content/GPT-SoVITS/TEMP/tmp_s2.json\"\n",
            "INFO:yyn:{'train': {'log_interval': 100, 'eval_interval': 500, 'seed': 1234, 'epochs': 8, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 7, 'fp16_run': True, 'lr_decay': 0.999875, 'segment_size': 20480, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'text_low_lr_rate': 0.4, 'pretrained_s2G': 'GPT_SoVITS/pretrained_models/s2G488k.pth', 'pretrained_s2D': 'GPT_SoVITS/pretrained_models/s2D488k.pth', 'if_save_latest': True, 'if_save_every_weights': True, 'save_every_epoch': 4, 'gpu_numbers': '0'}, 'data': {'max_wav_value': 32768.0, 'sampling_rate': 32000, 'filter_length': 2048, 'hop_length': 640, 'win_length': 2048, 'n_mel_channels': 128, 'mel_fmin': 0.0, 'mel_fmax': None, 'add_blank': True, 'n_speakers': 300, 'cleaned_text': True, 'exp_dir': 'logs/yyn'}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 8, 2, 2], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 512, 'semantic_frame_rate': '25hz', 'freeze_quantizer': True}, 's2_ckpt_dir': 'logs/yyn', 'content_module': 'cnhubert', 'save_weight_dir': 'SoVITS_weights', 'name': 'yyn', 'pretrain': None, 'resume_step': None}\n",
            "phoneme_data_len: 10\n",
            "wav_data_len: 100\n",
            "100% 100/100 [00:00<00:00, 55093.97it/s]\n",
            "skipped_phone:  0 , skipped_dur:  0\n",
            "total left:  100\n",
            "/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "ssl_proj.weight not requires_grad\n",
            "ssl_proj.bias not requires_grad\n",
            "INFO:yyn:loaded pretrained GPT_SoVITS/pretrained_models/s2G488k.pth\n",
            "<All keys matched successfully>\n",
            "INFO:yyn:loaded pretrained GPT_SoVITS/pretrained_models/s2D488k.pth\n",
            "<All keys matched successfully>\n",
            "/usr/local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:30.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
            "grad.sizes() = [1, 9, 96], strides() = [45600, 96, 1]\n",
            "bucket_view.sizes() = [1, 9, 96], strides() = [864, 96, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "INFO:yyn:Train Epoch: 1 [0%]\n",
            "INFO:yyn:[2.442312717437744, 2.5402657985687256, 9.606376647949219, 23.594749450683594, 0.6438313722610474, 2.652332305908203, 0, 9.99875e-05]\n",
            "15it [00:39,  2.61s/it]\n",
            "INFO:yyn:====> Epoch: 1\n",
            "15it [00:16,  1.07s/it]\n",
            "INFO:yyn:====> Epoch: 2\n",
            "15it [00:15,  1.04s/it]\n",
            "INFO:yyn:====> Epoch: 3\n",
            "15it [00:15,  1.02s/it]\n",
            "INFO:yyn:Saving model and optimizer state at iteration 4 to logs/yyn/logs_s2/G_233333333333.pth\n",
            "INFO:yyn:Saving model and optimizer state at iteration 4 to logs/yyn/logs_s2/D_233333333333.pth\n",
            "INFO:yyn:saving ckpt yyn_e4:Success.\n",
            "INFO:yyn:====> Epoch: 4\n",
            "15it [00:16,  1.09s/it]\n",
            "INFO:yyn:====> Epoch: 5\n",
            "15it [00:16,  1.12s/it]\n",
            "INFO:yyn:====> Epoch: 6\n",
            "10it [00:13,  1.49it/s]INFO:yyn:Train Epoch: 7 [67%]\n",
            "INFO:yyn:[2.245149612426758, 3.014669418334961, 12.739520072937012, 23.850242614746094, 0.5451493263244629, 1.5745354890823364, 100, 9.991253280566489e-05]\n",
            "15it [00:17,  1.14s/it]\n",
            "INFO:yyn:====> Epoch: 7\n",
            "15it [00:16,  1.10s/it]\n",
            "INFO:yyn:Saving model and optimizer state at iteration 8 to logs/yyn/logs_s2/G_233333333333.pth\n",
            "INFO:yyn:Saving model and optimizer state at iteration 8 to logs/yyn/logs_s2/D_233333333333.pth\n",
            "INFO:yyn:saving ckpt yyn_e8:Success.\n",
            "INFO:yyn:====> Epoch: 8\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/s1_train.py --config_file \"/content/GPT-SoVITS/TEMP/tmp_s1.yaml\" \n",
            "Seed set to 1234\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "<All keys matched successfully>\n",
            "ckpt_path: None\n",
            "[rank: 0] Seed set to 1234\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Missing logger folder: logs/yyn/logs_s1/logs_s1\n",
            "semantic_data_len: 10\n",
            "phoneme_data_len: 10\n",
            "                    item_name                                     semantic_audio\n",
            "0    3.flac_815360_933120.wav  582 677 391 337 620 270 911 196 645 983 663 59...\n",
            "1    3.flac_151040_313600.wav  54 524 256 101 980 922 246 513 68 330 586 918 ...\n",
            "2  3.flac_1063680_1308800.wav  1012 679 485 189 681 1015 406 639 388 724 860 ...\n",
            "3  3.flac_1322240_1455680.wav  171 295 179 557 488 276 589 195 822 369 441 98...\n",
            "4    3.flac_661120_797120.wav  23 341 535 69 156 156 156 156 834 834 428 576 ...\n",
            "5    3.flac_313600_521920.wav  214 200 756 896 104 834 944 699 834 417 273 41...\n",
            "6  3.flac_1463680_1615040.wav  214 357 441 155 324 819 123 164 957 422 939 71...\n",
            "7   3.flac_933120_1037120.wav  365 248 230 298 197 550 943 154 651 647 953 39...\n",
            "8         3.flac_0_136640.wav  520 53 760 535 10 774 420 428 958 312 641 844 ...\n",
            "9    3.flac_521920_661120.wav  623 221 764 156 237 350 549 556 199 205 52 515...\n",
            "dataset.__len__(): 100\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                 | Params\n",
            "-----------------------------------------------\n",
            "0 | model | Text2SemanticDecoder | 77.5 M\n",
            "-----------------------------------------------\n",
            "77.5 M    Trainable params\n",
            "0         Non-trainable params\n",
            "77.5 M    Total params\n",
            "309.975   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 14: 100% 15/15 [00:02<00:00,  7.24it/s, v_num=0, total_loss_step=65.50, lr_step=0.002, top_3_acc_step=1.000, total_loss_epoch=229.0, lr_epoch=0.002, top_3_acc_epoch=0.999]`Trainer.fit` stopped: `max_epochs=15` reached.\n",
            "Epoch 14: 100% 15/15 [00:07<00:00,  1.93it/s, v_num=0, total_loss_step=65.50, lr_step=0.002, top_3_acc_step=1.000, total_loss_epoch=229.0, lr_epoch=0.002, top_3_acc_epoch=0.999]\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/inference_webui.py\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg6\n",
            "DEBUG:torio._extension.utils:Successfully loaded FFmpeg6\n",
            "Some weights of the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "<All keys matched successfully>\n",
            "Number of parameter: 77.49M\n",
            "Running on local URL:  http://0.0.0.0:9872\n",
            "Running on public URL: https://06e0e3fdf9a2326487.gradio.live\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 十三年后，吉他手顾小白回老家担任刑侦队队长，在调查一系列死亡案件的时候，发现它们都隐隐指向多年前的孟海老师被杀案，随着调查的深入，他发现身边的每个人都牵涉其中……经过了十三个夏天，终于秘密得以层层剥离，露出贪欲的本质，然而这场针对欲望的清算，让所有的参与者几乎无一幸存。\n",
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba_fast:Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "DEBUG:jieba_fast:Loading model from cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Loading model cost 1.282 seconds.\n",
            "DEBUG:jieba_fast:Loading model cost 1.282 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "DEBUG:jieba_fast:Prefix dict has been built succesfully.\n",
            "[]\n",
            "实际输入的目标文本(切句后): 十三年后，吉他手顾小白回老家担任刑侦队队长，在调查一系列死亡案件的时候，发现它们都隐隐指向多年前的孟海老师被杀案，随着调查的深入，他发现身边的每个人都牵涉其中……经过了十三个夏天，终于秘密得以层层剥离，露出贪欲的本质，然而这场针对欲望的清算，让所有的参与者几乎无一幸存\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 十三年后，吉他手顾小白回老家担任刑侦队队长，在调查一系列死亡案件的时候，发现它们都隐隐指向多年前的孟海老师被杀案，随着调查的深入，他发现身边的每个人都牵涉其中……经过了十三个夏天，终于秘密得以层层剥离，露出贪欲的本质，然而这场针对欲望的清算，让所有的参与者几乎无一幸存。\n",
            "[]\n",
            "['十三年后，吉他手顾小白回老家担任刑侦队队长，在调查一系列死亡案件的时候，发现它们都隐隐指向多年前的孟海老师被杀案，随着调查的深入，他发现身边的每个人都牵涉其中……经过了十三个夏天，终于秘密得以层层剥离，露出贪欲的本质，然而这场针对欲望的清算，让所有的参与者几乎无一幸存。']\n",
            "['zh']\n",
            " 45% 671/1500 [00:11<00:17, 46.87it/s]T2S Decoding EOS [206 -> 880]\n",
            " 45% 674/1500 [00:11<00:13, 60.08it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "3.881\t1.417\t11.224\t1.351\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "[]\n",
            "实际输入的目标文本(切句后): 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "[]\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            " 11% 172/1500 [00:03<00:22, 59.85it/s]T2S Decoding EOS [206 -> 378]\n",
            " 11% 172/1500 [00:03<00:24, 53.93it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.264\t0.115\t3.190\t0.641\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 。真烦人！\n",
            "[]\n",
            "实际输入的目标文本(切句后): 真烦人！\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 真烦人！\n",
            "[]\n",
            "['真烦人！']\n",
            "['zh']\n",
            "  1% 20/1500 [00:00<00:25, 58.01it/s]T2S Decoding EOS [206 -> 228]\n",
            "  1% 22/1500 [00:00<00:26, 55.80it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.148\t0.058\t0.395\t0.908\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 晚上吃啥\n",
            "[]\n",
            "实际输入的目标文本(切句后): 晚上吃啥\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 晚上吃啥。\n",
            "[]\n",
            "['晚上吃啥。']\n",
            "['zh']\n",
            "  1% 18/1500 [00:00<00:32, 45.54it/s]T2S Decoding EOS [206 -> 229]\n",
            "  2% 23/1500 [00:00<00:33, 43.59it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.374\t0.100\t0.529\t0.469\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 明天我下黑回去，给我爸带着。还上咱家住去，然后给他也办个暂住证。最后以后我肯定天天吃完饭就把厨房收拾干净儿的，不等到第二天。\n",
            "[]\n",
            "实际输入的目标文本(切句后): 明天我下黑回去，给我爸带着\n",
            "还上咱家住去，然后给他也办个暂住证\n",
            "最后以后我肯定天天吃完饭就把厨房收拾干净儿的，不等到第二天\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 明天我下黑回去，给我爸带着。\n",
            "[]\n",
            "['明天我下黑回去，给我爸带着。']\n",
            "['zh']\n",
            "  4% 59/1500 [00:01<00:35, 41.06it/s]T2S Decoding EOS [206 -> 265]\n",
            "  4% 59/1500 [00:01<00:36, 38.97it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 还上咱家住去，然后给他也办个暂住证。\n",
            "[]\n",
            "['还上咱家住去，然后给他也办个暂住证。']\n",
            "['zh']\n",
            "  4% 63/1500 [00:00<00:22, 63.91it/s]T2S Decoding EOS [206 -> 274]\n",
            "  5% 68/1500 [00:01<00:22, 62.89it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 最后以后我肯定天天吃完饭就把厨房收拾干净儿的，不等到第二天。\n",
            "[]\n",
            "['最后以后我肯定天天吃完饭就把厨房收拾干净儿的，不等到第二天。']\n",
            "['zh']\n",
            "  7% 104/1500 [00:01<00:22, 61.52it/s]T2S Decoding EOS [206 -> 315]\n",
            "  7% 109/1500 [00:01<00:22, 61.39it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.285\t3.818\t1.777\t0.494\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 明天我下黑回去，给我爸带着。还上咱家住切，然后给他也办个暂住证。最后以后我肯定天天吃完饭就把厨房收拾干净儿的，不等到第二天。\n",
            "[]\n",
            "实际输入的目标文本(切句后): 明天我下黑回去，给我爸带着\n",
            "还上咱家住切，然后给他也办个暂住证\n",
            "最后以后我肯定天天吃完饭就把厨房收拾干净儿的，不等到第二天\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 明天我下黑回去，给我爸带着。\n",
            "[]\n",
            "['明天我下黑回去，给我爸带着。']\n",
            "['zh']\n",
            "  4% 61/1500 [00:01<00:31, 46.19it/s]T2S Decoding EOS [206 -> 268]\n",
            "  4% 62/1500 [00:01<00:32, 44.58it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 还上咱家住切，然后给他也办个暂住证。\n",
            "[]\n",
            "['还上咱家住切，然后给他也办个暂住证。']\n",
            "['zh']\n",
            "  5% 81/1500 [00:02<00:34, 40.86it/s]T2S Decoding EOS [206 -> 290]\n",
            "  6% 84/1500 [00:02<00:36, 39.26it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 最后以后我肯定天天吃完饭就把厨房收拾干净儿的，不等到第二天。\n",
            "[]\n",
            "['最后以后我肯定天天吃完饭就把厨房收拾干净儿的，不等到第二天。']\n",
            "['zh']\n",
            " 10% 145/1500 [00:02<00:22, 61.46it/s]T2S Decoding EOS [206 -> 351]\n",
            " 10% 145/1500 [00:02<00:22, 60.51it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.153\t4.962\t2.397\t0.490\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 明天我下黑回去，给我爸带着。还上咱家住，然后给他也办个暂住证。以后我肯定天天吃完饭就把厨房收拾干净，不等第二天。\n",
            "[]\n",
            "实际输入的目标文本(切句后): 明天我下黑回去，给我爸带着\n",
            "还上咱家住，然后给他也办个暂住证\n",
            "以后我肯定天天吃完饭就把厨房收拾干净，不等第二天\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 明天我下黑回去，给我爸带着。\n",
            "[]\n",
            "['明天我下黑回去，给我爸带着。']\n",
            "['zh']\n",
            "  4% 56/1500 [00:00<00:23, 62.75it/s]T2S Decoding EOS [206 -> 265]\n",
            "  4% 59/1500 [00:00<00:23, 61.47it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 还上咱家住，然后给他也办个暂住证。\n",
            "[]\n",
            "['还上咱家住，然后给他也办个暂住证。']\n",
            "['zh']\n",
            "  4% 61/1500 [00:01<00:22, 62.93it/s]T2S Decoding EOS [206 -> 268]\n",
            "  4% 62/1500 [00:01<00:24, 59.87it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 以后我肯定天天吃完饭就把厨房收拾干净，不等第二天。\n",
            "[]\n",
            "['以后我肯定天天吃完饭就把厨房收拾干净，不等第二天。']\n",
            "['zh']\n",
            "  7% 98/1500 [00:01<00:21, 64.69it/s]T2S Decoding EOS [206 -> 307]\n",
            "  7% 101/1500 [00:01<00:22, 62.56it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.187\t2.650\t1.616\t0.499\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 我记着我欠你一万块钱，最近我宽敞点儿就给你。\n",
            "[]\n",
            "实际输入的目标文本(切句后): 我记着我欠你一万块钱，最近我宽敞点儿就给你\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 我记着我欠你一万块钱，最近我宽敞点儿就给你。\n",
            "[]\n",
            "['我记着我欠你一万块钱，最近我宽敞点儿就给你。']\n",
            "['zh']\n",
            "  6% 84/1500 [00:01<00:22, 63.79it/s]T2S Decoding EOS [206 -> 295]\n",
            "  6% 89/1500 [00:01<00:22, 62.57it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.159\t0.066\t1.424\t0.499\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 。我记着，我欠你一万块钱，最近我宽敞点儿就给你。\n",
            "[]\n",
            "实际输入的目标文本(切句后): 我记着，我欠你一万块钱，最近我宽敞点儿就给你\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 我记着，我欠你一万块钱，最近我宽敞点儿就给你。\n",
            "[]\n",
            "['我记着，我欠你一万块钱，最近我宽敞点儿就给你。']\n",
            "['zh']\n",
            "  7% 98/1500 [00:01<00:22, 62.63it/s]T2S Decoding EOS [206 -> 304]\n",
            "  7% 98/1500 [00:01<00:22, 63.24it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.209\t0.068\t1.551\t0.496\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 今儿个给你做黄瓜片儿炒鸡蛋。\n",
            "[]\n",
            "实际输入的目标文本(切句后): 今儿个给你做黄瓜片儿炒鸡蛋\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 今儿个给你做黄瓜片儿炒鸡蛋。\n",
            "[]\n",
            "['今儿个给你做黄瓜片儿炒鸡蛋。']\n",
            "['zh']\n",
            "  3% 48/1500 [00:00<00:23, 60.53it/s]T2S Decoding EOS [206 -> 258]\n",
            "  3% 52/1500 [00:00<00:24, 60.26it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.160\t0.071\t0.864\t0.524\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 。今儿个给你做黄瓜片儿炒鸡蛋。\n",
            "[]\n",
            "实际输入的目标文本(切句后): 今儿个给你做黄瓜片儿炒鸡蛋\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 今儿个给你做黄瓜片儿炒鸡蛋。\n",
            "[]\n",
            "['今儿个给你做黄瓜片儿炒鸡蛋。']\n",
            "['zh']\n",
            "  3% 52/1500 [00:01<00:27, 53.04it/s]T2S Decoding EOS [206 -> 261]\n",
            "  4% 55/1500 [00:01<00:32, 44.92it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.237\t0.102\t1.226\t0.472\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 。坐这儿嘎达，我还没说完那！\n",
            "[]\n",
            "实际输入的目标文本(切句后): 坐这儿嘎达，我还没说完那！\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 坐这儿嘎达，我还没说完那！\n",
            "[]\n",
            "['坐这儿嘎达，我还没说完那！']\n",
            "['zh']\n",
            "  3% 46/1500 [00:00<00:23, 62.19it/s]T2S Decoding EOS [206 -> 254]\n",
            "  3% 48/1500 [00:00<00:24, 59.54it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.157\t0.072\t0.807\t0.543\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 。坐这儿嘎达！我还没说完那！\n",
            "[]\n",
            "实际输入的目标文本(切句后): 坐这儿嘎达！我还没说完那！\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 坐这儿嘎达！我还没说完那！\n",
            "[]\n",
            "['坐这儿嘎达！我还没说完那！']\n",
            "['zh']\n",
            "  4% 55/1500 [00:00<00:23, 62.79it/s]T2S Decoding EOS [206 -> 264]\n",
            "  4% 58/1500 [00:00<00:23, 60.77it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.215\t0.066\t0.955\t0.499\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 坐这儿嘎达！我还没说完那！\n",
            "[]\n",
            "实际输入的目标文本(切句后): 坐这儿嘎达！我还没说完那！\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 坐这儿嘎达！我还没说完那！\n",
            "[]\n",
            "['坐这儿嘎达！我还没说完那！']\n",
            "['zh']\n",
            "  3% 46/1500 [00:00<00:24, 60.51it/s]T2S Decoding EOS [206 -> 258]\n",
            "  3% 52/1500 [00:00<00:24, 58.74it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.149\t0.063\t0.886\t0.267\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 。坐！这儿嘎达！我还没说完那！\n",
            "[]\n",
            "实际输入的目标文本(切句后): 坐！这儿嘎达！我还没说完那！\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 坐！这儿嘎达！我还没说完那！\n",
            "[]\n",
            "['坐！这儿嘎达！我还没说完那！']\n",
            "['zh']\n",
            "  5% 75/1500 [00:01<00:27, 51.19it/s]T2S Decoding EOS [206 -> 285]\n",
            "  5% 79/1500 [00:01<00:25, 56.73it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.180\t0.083\t1.394\t0.792\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 。坐！这儿嘎达！我还没说完那！\n",
            "[]\n",
            "实际输入的目标文本(切句后): 。坐！这儿嘎达！我还没说完那！\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 。坐！这儿嘎达！我还没说完那！\n",
            "[]\n",
            "['。坐！这儿嘎达！我还没说完那！']\n",
            "['zh']\n",
            "  5% 69/1500 [00:01<00:22, 63.71it/s]T2S Decoding EOS [206 -> 276]\n",
            "  5% 70/1500 [00:01<00:23, 61.36it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.210\t0.063\t1.142\t0.500\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 坐这儿嘎达！我还没说完那！\n",
            "[]\n",
            "实际输入的目标文本(切句后): 坐这儿嘎达！我还没说完那！\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 坐这儿嘎达！我还没说完那！\n",
            "[]\n",
            "['坐这儿嘎达！我还没说完那！']\n",
            "['zh']\n",
            "  3% 42/1500 [00:00<00:24, 60.63it/s]T2S Decoding EOS [206 -> 253]\n",
            "  3% 47/1500 [00:00<00:24, 60.25it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.172\t0.063\t0.781\t0.483\n",
            "实际输入的参考文本: 有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。\n",
            "实际输入的目标文本: 坐这儿嘎达！我还没说完那！\n",
            "[]\n",
            "实际输入的目标文本(切句后): 坐这儿嘎达！我还没说完那！\n",
            "['有时候他提前打电话，有时候不提前打电话，就得是那个街道上，咱楼上还拍照，还拍视频呢。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 坐这儿嘎达！我还没说完那！\n",
            "[]\n",
            "['坐这儿嘎达！我还没说完那！']\n",
            "['zh']\n",
            "  3% 41/1500 [00:00<00:22, 64.24it/s]T2S Decoding EOS [206 -> 251]\n",
            "  3% 45/1500 [00:00<00:23, 61.82it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.157\t0.061\t0.729\t0.512\n",
            "\"/usr/local/bin/python\" tools/slice_audio.py \"/content/GPT-SoVITS/1.wav\" \"output/slicer_opt\" -34 4000 300 10 500 0.9 0.25 0 1\n",
            "执行完毕，请检查输出文件\n",
            "\"/usr/local/bin/python\" tools/damo_asr/cmd-asr.py \"/content/GPT-SoVITS/output/slicer_opt\"\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "  0% 0/1 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"/content/GPT-SoVITS/tools/damo_asr/cmd-asr.py\", line 30, in <module>\n",
            "    text = model.generate(input=\"%s/%s\"%(dir,name))[0][\"text\"]\n",
            "  File \"/usr/local/lib/python3.9/site-packages/funasr/auto/auto_model.py\", line 206, in generate\n",
            "    return self.inference_with_vad(input, input_len=input_len, **cfg)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/funasr/auto/auto_model.py\", line 270, in inference_with_vad\n",
            "    res = self.inference(input, input_len=input_len, model=self.vad_model, kwargs=self.vad_kwargs, **cfg)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/funasr/auto/auto_model.py\", line 237, in inference\n",
            "    results, meta_data = model.inference(**batch, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/funasr/models/fsmn_vad_streaming/model.py\", line 579, in inference\n",
            "    audio_sample_list = load_audio_text_image_video(data_in,\n",
            "  File \"/usr/local/lib/python3.9/site-packages/funasr/utils/load_utils.py\", line 34, in load_audio_text_image_video\n",
            "    return [load_audio_text_image_video(audio, fs=fs, audio_fs=audio_fs, data_type=data_type, **kwargs) for audio in data_or_path_or_list]\n",
            "  File \"/usr/local/lib/python3.9/site-packages/funasr/utils/load_utils.py\", line 34, in <listcomp>\n",
            "    return [load_audio_text_image_video(audio, fs=fs, audio_fs=audio_fs, data_type=data_type, **kwargs) for audio in data_or_path_or_list]\n",
            "  File \"/usr/local/lib/python3.9/site-packages/funasr/utils/load_utils.py\", line 41, in load_audio_text_image_video\n",
            "    data_or_path_or_list, audio_fs = torchaudio.load(data_or_path_or_list)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torchaudio/_backend/utils.py\", line 205, in load\n",
            "    return backend.load(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torchaudio/_backend/ffmpeg.py\", line 297, in load\n",
            "    return load_audio(uri, frame_offset, num_frames, normalize, channels_first, format)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torchaudio/_backend/ffmpeg.py\", line 88, in load_audio\n",
            "    s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torio/io/_streaming_media_decoder.py\", line 526, in __init__\n",
            "    self._be = ffmpeg_ext.StreamingMediaDecoder(os.path.normpath(src), format, option)\n",
            "RuntimeError: Failed to open the input \"/content/GPT-SoVITS/output/slicer_opt/.ipynb_checkpoints\" (Is a directory).\n",
            "Exception raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\n",
            "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x799e58f81d87 in /usr/local/lib/python3.9/site-packages/torch/lib/libc10.so)\n",
            "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x799e58f3275f in /usr/local/lib/python3.9/site-packages/torch/lib/libc10.so)\n",
            "frame #2: <unknown function> + 0x42904 (0x799d93598904 in /usr/local/lib/python3.9/site-packages/torio/lib/libtorio_ffmpeg6.so)\n",
            "frame #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x799d9359b304 in /usr/local/lib/python3.9/site-packages/torio/lib/libtorio_ffmpeg6.so)\n",
            "frame #4: <unknown function> + 0x3a6fe (0x799d896ec6fe in /usr/local/lib/python3.9/site-packages/torio/lib/_torio_ffmpeg6.so)\n",
            "frame #5: <unknown function> + 0x32175 (0x799d896e4175 in /usr/local/lib/python3.9/site-packages/torio/lib/_torio_ffmpeg6.so)\n",
            "frame #6: /usr/local/bin/python() [0x507387]\n",
            "frame #7: _PyObject_MakeTpCall + 0x2ec (0x4f073c in /usr/local/bin/python)\n",
            "frame #8: /usr/local/bin/python() [0x505313]\n",
            "frame #9: /usr/local/bin/python() [0x502a80]\n",
            "frame #10: /usr/local/bin/python() [0x4f0bea]\n",
            "frame #11: <unknown function> + 0xf294 (0x799d9361d294 in /usr/local/lib/python3.9/site-packages/torchaudio/lib/_torchaudio.so)\n",
            "frame #12: _PyObject_MakeTpCall + 0x2ec (0x4f073c in /usr/local/bin/python)\n",
            "frame #13: _PyEval_EvalFrameDefault + 0x5263 (0x4ecc93 in /usr/local/bin/python)\n",
            "frame #14: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #15: _PyObject_FastCallDictTstate + 0x13e (0x4effae in /usr/local/bin/python)\n",
            "frame #16: /usr/local/bin/python() [0x5027bf]\n",
            "frame #17: _PyObject_MakeTpCall + 0x303 (0x4f0753 in /usr/local/bin/python)\n",
            "frame #18: _PyEval_EvalFrameDefault + 0x5263 (0x4ecc93 in /usr/local/bin/python)\n",
            "frame #19: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #20: _PyFunction_Vectorcall + 0xd4 (0x4f7e54 in /usr/local/bin/python)\n",
            "frame #21: _PyEval_EvalFrameDefault + 0x3c7 (0x4e7df7 in /usr/local/bin/python)\n",
            "frame #22: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #23: _PyFunction_Vectorcall + 0xd4 (0x4f7e54 in /usr/local/bin/python)\n",
            "frame #24: _PyEval_EvalFrameDefault + 0x4d34 (0x4ec764 in /usr/local/bin/python)\n",
            "frame #25: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #26: _PyFunction_Vectorcall + 0xd4 (0x4f7e54 in /usr/local/bin/python)\n",
            "frame #27: _PyEval_EvalFrameDefault + 0x4d34 (0x4ec764 in /usr/local/bin/python)\n",
            "frame #28: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #29: _PyFunction_Vectorcall + 0xd4 (0x4f7e54 in /usr/local/bin/python)\n",
            "frame #30: PyObject_Call + 0xb4 (0x5057d4 in /usr/local/bin/python)\n",
            "frame #31: _PyEval_EvalFrameDefault + 0x3e14 (0x4eb844 in /usr/local/bin/python)\n",
            "frame #32: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #33: _PyFunction_Vectorcall + 0xd4 (0x4f7e54 in /usr/local/bin/python)\n",
            "frame #34: _PyEval_EvalFrameDefault + 0x3c7 (0x4e7df7 in /usr/local/bin/python)\n",
            "frame #35: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #36: _PyFunction_Vectorcall + 0xd4 (0x4f7e54 in /usr/local/bin/python)\n",
            "frame #37: _PyEval_EvalFrameDefault + 0x1231 (0x4e8c61 in /usr/local/bin/python)\n",
            "frame #38: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #39: /usr/local/bin/python() [0x50508d]\n",
            "frame #40: PyObject_Call + 0xb4 (0x5057d4 in /usr/local/bin/python)\n",
            "frame #41: _PyEval_EvalFrameDefault + 0x3e14 (0x4eb844 in /usr/local/bin/python)\n",
            "frame #42: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #43: /usr/local/bin/python() [0x50508d]\n",
            "frame #44: PyObject_Call + 0xb4 (0x5057d4 in /usr/local/bin/python)\n",
            "frame #45: _PyEval_EvalFrameDefault + 0x3e14 (0x4eb844 in /usr/local/bin/python)\n",
            "frame #46: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #47: /usr/local/bin/python() [0x50508d]\n",
            "frame #48: PyObject_Call + 0xb4 (0x5057d4 in /usr/local/bin/python)\n",
            "frame #49: _PyEval_EvalFrameDefault + 0x3e14 (0x4eb844 in /usr/local/bin/python)\n",
            "frame #50: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #51: /usr/local/bin/python() [0x50508d]\n",
            "frame #52: _PyEval_EvalFrameDefault + 0x1231 (0x4e8c61 in /usr/local/bin/python)\n",
            "frame #53: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #54: _PyEval_EvalCodeWithName + 0x47 (0x4e67b7 in /usr/local/bin/python)\n",
            "frame #55: PyEval_EvalCodeEx + 0x39 (0x4e6769 in /usr/local/bin/python)\n",
            "frame #56: PyEval_EvalCode + 0x1b (0x59466b in /usr/local/bin/python)\n",
            "frame #57: /usr/local/bin/python() [0x5c1dc7]\n",
            "frame #58: /usr/local/bin/python() [0x5bddd0]\n",
            "frame #59: /usr/local/bin/python() [0x45674e]\n",
            "frame #60: PyRun_SimpleFileExFlags + 0x1a2 (0x5b7ab2 in /usr/local/bin/python)\n",
            "frame #61: Py_RunMain + 0x37e (0x5b502e in /usr/local/bin/python)\n",
            "frame #62: Py_BytesMain + 0x39 (0x588719 in /usr/local/bin/python)\n",
            "frame #63: <unknown function> + 0x29d90 (0x799e5a0e7d90 in /lib/x86_64-linux-gnu/libc.so.6)\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "rtf_avg: 0.085: 100% 1/1 [00:00<00:00,  3.35it/s]\n",
            "time cost vad: 0.299\n",
            "rtf_avg: 0.279, time_speech:  3.530, time_escape: 0.985: 100% 1/1 [00:00<00:00,  1.00it/s]\n",
            "rtf_avg: 0.015: 100% 1/1 [00:00<00:00, 14.26it/s]\n",
            "time cost vad: 0.070\n",
            "rtf_avg: 0.022, time_speech:  4.780, time_escape: 0.104: 100% 1/1 [00:00<00:00,  8.56it/s]\n",
            "\"/usr/local/bin/python\" tools/subfix_webui.py --load_list \"/content/GPT-SoVITS/output/asr_opt/slicer_opt.list\" --webui_port 9871 --is_share True\n",
            "/content/GPT-SoVITS/tools/subfix_webui.py:366: GradioUnusedKwargWarning: You have unused kwarg parameters in Button, please remove them: {'link': '?__theme=light'}\n",
            "  btn_theme_dark = gr.Button(\"Light Theme\", link=\"?__theme=light\", scale=1)\n",
            "/content/GPT-SoVITS/tools/subfix_webui.py:367: GradioUnusedKwargWarning: You have unused kwarg parameters in Button, please remove them: {'link': '?__theme=dark'}\n",
            "  btn_theme_light = gr.Button(\"Dark Theme\", link=\"?__theme=dark\", scale=1)\n",
            "Running on local URL:  http://0.0.0.0:9871\n",
            "Running on public URL: https://74b5e82f2c000b086b.gradio.live\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/s2_train.py --config \"/content/GPT-SoVITS/TEMP/tmp_s2.json\"\n",
            "INFO:wyr:{'train': {'log_interval': 100, 'eval_interval': 500, 'seed': 1234, 'epochs': 8, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 7, 'fp16_run': True, 'lr_decay': 0.999875, 'segment_size': 20480, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'text_low_lr_rate': 0.4, 'pretrained_s2G': 'GPT_SoVITS/pretrained_models/s2G488k.pth', 'pretrained_s2D': 'GPT_SoVITS/pretrained_models/s2D488k.pth', 'if_save_latest': True, 'if_save_every_weights': True, 'save_every_epoch': 4, 'gpu_numbers': '0'}, 'data': {'max_wav_value': 32768.0, 'sampling_rate': 32000, 'filter_length': 2048, 'hop_length': 640, 'win_length': 2048, 'n_mel_channels': 128, 'mel_fmin': 0.0, 'mel_fmax': None, 'add_blank': True, 'n_speakers': 300, 'cleaned_text': True, 'exp_dir': 'logs/wyr'}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 8, 2, 2], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 512, 'semantic_frame_rate': '25hz', 'freeze_quantizer': True}, 's2_ckpt_dir': 'logs/wyr', 'content_module': 'cnhubert', 'save_weight_dir': 'SoVITS_weights', 'name': 'wyr', 'pretrain': None, 'resume_step': None}\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/s2_train.py\", line 600, in <module>\n",
            "    main()\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/s2_train.py\", line 56, in main\n",
            "    mp.spawn(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 241, in spawn\n",
            "    return start_processes(fn, args, nprocs, join, daemon, start_method=\"spawn\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 197, in start_processes\n",
            "    while not context.join():\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 158, in join\n",
            "    raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
            "torch.multiprocessing.spawn.ProcessRaisedException: \n",
            "\n",
            "-- Process 0 terminated with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 68, in _wrap\n",
            "    fn(i, *args)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/s2_train.py\", line 85, in run\n",
            "    train_dataset = TextAudioSpeakerLoader(hps.data)  ########\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/module/data_utils.py\", line 35, in __init__\n",
            "    assert os.path.exists(self.path2)\n",
            "AssertionError\n",
            "\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Loading model cost 1.874 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "Loading model cost 2.045 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py\n",
            "Some weights of the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/3-get-semantic.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/3-get-semantic.py\n",
            "/usr/local/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "/usr/local/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/s2_train.py --config \"/content/GPT-SoVITS/TEMP/tmp_s2.json\"\n",
            "INFO:wyr:{'train': {'log_interval': 100, 'eval_interval': 500, 'seed': 1234, 'epochs': 8, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 7, 'fp16_run': True, 'lr_decay': 0.999875, 'segment_size': 20480, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'text_low_lr_rate': 0.4, 'pretrained_s2G': 'GPT_SoVITS/pretrained_models/s2G488k.pth', 'pretrained_s2D': 'GPT_SoVITS/pretrained_models/s2D488k.pth', 'if_save_latest': True, 'if_save_every_weights': True, 'save_every_epoch': 4, 'gpu_numbers': '0'}, 'data': {'max_wav_value': 32768.0, 'sampling_rate': 32000, 'filter_length': 2048, 'hop_length': 640, 'win_length': 2048, 'n_mel_channels': 128, 'mel_fmin': 0.0, 'mel_fmax': None, 'add_blank': True, 'n_speakers': 300, 'cleaned_text': True, 'exp_dir': 'logs/wyr'}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 8, 2, 2], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 512, 'semantic_frame_rate': '25hz', 'freeze_quantizer': True}, 's2_ckpt_dir': 'logs/wyr', 'content_module': 'cnhubert', 'save_weight_dir': 'SoVITS_weights', 'name': 'wyr', 'pretrain': None, 'resume_step': None}\n",
            "phoneme_data_len: 2\n",
            "wav_data_len: 100\n",
            "100% 100/100 [00:00<00:00, 96221.70it/s]\n",
            "skipped_phone:  0 , skipped_dur:  0\n",
            "total left:  100\n",
            "/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "ssl_proj.weight not requires_grad\n",
            "ssl_proj.bias not requires_grad\n",
            "INFO:wyr:loaded pretrained GPT_SoVITS/pretrained_models/s2G488k.pth\n",
            "<All keys matched successfully>\n",
            "INFO:wyr:loaded pretrained GPT_SoVITS/pretrained_models/s2D488k.pth\n",
            "<All keys matched successfully>\n",
            "/usr/local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:30.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
            "grad.sizes() = [1, 9, 96], strides() = [45984, 96, 1]\n",
            "bucket_view.sizes() = [1, 9, 96], strides() = [864, 96, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "INFO:wyr:Train Epoch: 1 [0%]\n",
            "INFO:wyr:[2.874882698059082, 3.2574636936187744, 8.536559104919434, 23.47777557373047, 0.5293617248535156, 2.0973238945007324, 0, 9.99875e-05]\n",
            "15it [00:38,  2.56s/it]\n",
            "INFO:wyr:====> Epoch: 1\n",
            "15it [00:15,  1.02s/it]\n",
            "INFO:wyr:====> Epoch: 2\n",
            "15it [00:15,  1.02s/it]\n",
            "INFO:wyr:====> Epoch: 3\n",
            "15it [00:15,  1.02s/it]\n",
            "INFO:wyr:Saving model and optimizer state at iteration 4 to logs/wyr/logs_s2/G_233333333333.pth\n",
            "INFO:wyr:Saving model and optimizer state at iteration 4 to logs/wyr/logs_s2/D_233333333333.pth\n",
            "INFO:wyr:saving ckpt wyr_e4:Success.\n",
            "INFO:wyr:====> Epoch: 4\n",
            "15it [00:16,  1.10s/it]\n",
            "INFO:wyr:====> Epoch: 5\n",
            "15it [00:15,  1.06s/it]\n",
            "INFO:wyr:====> Epoch: 6\n",
            "10it [00:11,  1.47it/s]INFO:wyr:Train Epoch: 7 [67%]\n",
            "INFO:wyr:[2.0263328552246094, 2.886258363723755, 8.243431091308594, 17.185678482055664, 0.22196519374847412, 0.35252058506011963, 100, 9.991253280566489e-05]\n",
            "15it [00:16,  1.11s/it]\n",
            "INFO:wyr:====> Epoch: 7\n",
            "15it [00:15,  1.02s/it]\n",
            "INFO:wyr:Saving model and optimizer state at iteration 8 to logs/wyr/logs_s2/G_233333333333.pth\n",
            "INFO:wyr:Saving model and optimizer state at iteration 8 to logs/wyr/logs_s2/D_233333333333.pth\n",
            "INFO:wyr:saving ckpt wyr_e8:Success.\n",
            "INFO:wyr:====> Epoch: 8\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/s1_train.py --config_file \"/content/GPT-SoVITS/TEMP/tmp_s1.yaml\" \n",
            "Seed set to 1234\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "<All keys matched successfully>\n",
            "ckpt_path: None\n",
            "[rank: 0] Seed set to 1234\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Missing logger folder: logs/wyr/logs_s1/logs_s1\n",
            "semantic_data_len: 2\n",
            "phoneme_data_len: 2\n",
            "                 item_name                                     semantic_audio\n",
            "0  1.wav_182720_295680.wav  474 898 68 710 823 678 672 471 1009 112 446 62...\n",
            "1   1.wav_29760_182720.wav  208 2 177 932 551 494 298 783 416 1008 330 755...\n",
            "dataset.__len__(): 100\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                 | Params\n",
            "-----------------------------------------------\n",
            "0 | model | Text2SemanticDecoder | 77.5 M\n",
            "-----------------------------------------------\n",
            "77.5 M    Trainable params\n",
            "0         Non-trainable params\n",
            "77.5 M    Total params\n",
            "309.975   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 14: 100% 15/15 [00:01<00:00,  9.65it/s, v_num=0, total_loss_step=3.760, lr_step=0.002, top_3_acc_step=1.000, total_loss_epoch=16.30, lr_epoch=0.002, top_3_acc_epoch=1.000]`Trainer.fit` stopped: `max_epochs=15` reached.\n",
            "Epoch 14: 100% 15/15 [00:19<00:00,  1.28s/it, v_num=0, total_loss_step=3.760, lr_step=0.002, top_3_acc_step=1.000, total_loss_epoch=16.30, lr_epoch=0.002, top_3_acc_epoch=1.000]\n",
            "Number of parameter: 77.49M\n",
            "/usr/local/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "<All keys matched successfully>\n",
            "实际输入的参考文本: 我就一直学嘛，牛仔式，牛仔的连体。\n",
            "实际输入的目标文本: 不要脸的，你咋又抽上了呢。\n",
            "[]\n",
            "实际输入的目标文本(切句后): 不要脸的，你咋又抽上了呢\n",
            "['我就一直学嘛，牛仔式，牛仔的连体。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 不要脸的，你咋又抽上了呢。\n",
            "[]\n",
            "['不要脸的，你咋又抽上了呢。']\n",
            "['zh']\n",
            "  4% 57/1500 [00:01<00:26, 54.50it/s]T2S Decoding EOS [103 -> 160]\n",
            "  4% 57/1500 [00:01<00:32, 44.07it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.175\t0.101\t1.297\t0.561\n",
            "实际输入的参考文本: 我就一直学嘛，牛仔式，牛仔的连体。\n",
            "实际输入的目标文本: 。不要脸的，你咋又抽上了呢！\n",
            "[]\n",
            "实际输入的目标文本(切句后): 不要脸的，你咋又抽上了呢！\n",
            "['我就一直学嘛，牛仔式，牛仔的连体。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 不要脸的，你咋又抽上了呢！\n",
            "[]\n",
            "['不要脸的，你咋又抽上了呢！']\n",
            "['zh']\n",
            "  6% 95/1500 [00:01<00:23, 58.99it/s]T2S Decoding EOS [103 -> 201]\n",
            "  7% 98/1500 [00:01<00:23, 58.63it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.084\t0.058\t1.673\t0.308\n",
            "实际输入的参考文本: 我就一直学嘛，牛仔式，牛仔的连体。\n",
            "实际输入的目标文本: 。不要脸的，你咋又抽上了呢！\n",
            "[]\n",
            "实际输入的目标文本(切句后): 。不要脸的，你咋又抽上了呢！\n",
            "['我就一直学嘛，牛仔式，牛仔的连体。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 。不要脸的，你咋又抽上了呢！\n",
            "[]\n",
            "['。不要脸的，你咋又抽上了呢！']\n",
            "['zh']\n",
            "  4% 61/1500 [00:01<00:30, 46.45it/s]T2S Decoding EOS [103 -> 168]\n",
            "  4% 65/1500 [00:01<00:27, 52.14it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.078\t0.058\t1.248\t0.820\n",
            "实际输入的参考文本: 我就一直学嘛，牛仔式，牛仔的连体。\n",
            "实际输入的目标文本: 。不要脸的，你咋又抽上了呢！\n",
            "[]\n",
            "实际输入的目标文本(切句后): 不要脸的，你咋又抽上了呢！\n",
            "['我就一直学嘛，牛仔式，牛仔的连体。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 不要脸的，你咋又抽上了呢！\n",
            "[]\n",
            "['不要脸的，你咋又抽上了呢！']\n",
            "['zh']\n",
            "  4% 58/1500 [00:01<00:30, 47.03it/s]T2S Decoding EOS [103 -> 164]\n",
            "  4% 61/1500 [00:01<00:27, 51.55it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.085\t0.054\t1.184\t0.851\n",
            "实际输入的参考文本: 我就一直学嘛，牛仔式，牛仔的连体。\n",
            "实际输入的目标文本: 。亚楠那，你欠高超的欠还没还呢吧。\n",
            "[]\n",
            "实际输入的目标文本(切句后): 亚楠那，你欠高超的欠还没还呢吧\n",
            "['我就一直学嘛，牛仔式，牛仔的连体。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 亚楠那，你欠高超的欠还没还呢吧。\n",
            "[]\n",
            "['亚楠那，你欠高超的欠还没还呢吧。']\n",
            "['zh']\n",
            "  6% 90/1500 [00:02<00:34, 41.20it/s]T2S Decoding EOS [103 -> 197]\n",
            "  6% 94/1500 [00:02<00:34, 41.17it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.133\t0.084\t2.285\t0.902\n",
            "实际输入的参考文本: 我就一直学嘛，牛仔式，牛仔的连体。\n",
            "实际输入的目标文本: 。亚楠那。你欠高超的钱还没还呢吧。\n",
            "[]\n",
            "实际输入的目标文本(切句后): 亚楠那\n",
            "你欠高超的钱还没还呢吧\n",
            "['我就一直学嘛，牛仔式，牛仔的连体。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 亚楠那。\n",
            "[]\n",
            "['亚楠那。']\n",
            "['zh']\n",
            "  1% 19/1500 [00:00<00:25, 57.28it/s]T2S Decoding EOS [103 -> 122]\n",
            "  1% 19/1500 [00:00<00:27, 53.76it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 你欠高超的钱还没还呢吧。\n",
            "[]\n",
            "['你欠高超的钱还没还呢吧。']\n",
            "['zh']\n",
            "  3% 49/1500 [00:00<00:22, 63.30it/s]T2S Decoding EOS [103 -> 154]\n",
            "  3% 51/1500 [00:00<00:23, 60.77it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.087\t0.982\t0.840\t0.564\n",
            "实际输入的参考文本: 我就一直学嘛，牛仔式，牛仔的连体。\n",
            "实际输入的目标文本: 。亚楠那。你欠高超儿的钱还没还呢吧。\n",
            "[]\n",
            "实际输入的目标文本(切句后): 亚楠那\n",
            "你欠高超儿的钱还没还呢吧\n",
            "['我就一直学嘛，牛仔式，牛仔的连体。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 亚楠那。\n",
            "[]\n",
            "['亚楠那。']\n",
            "['zh']\n",
            "  1% 22/1500 [00:00<00:35, 41.38it/s]T2S Decoding EOS [103 -> 127]\n",
            "  2% 24/1500 [00:00<00:36, 40.76it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 你欠高超儿的钱还没还呢吧。\n",
            "[]\n",
            "['你欠高超儿的钱还没还呢吧。']\n",
            "['zh']\n",
            "  3% 49/1500 [00:01<00:34, 41.80it/s]T2S Decoding EOS [103 -> 156]\n",
            "  4% 53/1500 [00:01<00:36, 39.91it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.094\t1.489\t1.329\t0.547\n",
            "实际输入的参考文本: 我就一直学嘛，牛仔式，牛仔的连体。\n",
            "实际输入的目标文本: 。亚楠哪。你欠高超儿的钱还没还呢吧。\n",
            "[]\n",
            "实际输入的目标文本(切句后): 亚楠哪\n",
            "你欠高超儿的钱还没还呢吧\n",
            "['我就一直学嘛，牛仔式，牛仔的连体。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 亚楠哪。\n",
            "[]\n",
            "['亚楠哪。']\n",
            "['zh']\n",
            "  1% 18/1500 [00:00<00:25, 57.28it/s]T2S Decoding EOS [103 -> 125]\n",
            "  1% 22/1500 [00:00<00:27, 54.70it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 你欠高超儿的钱还没还呢吧。\n",
            "[]\n",
            "['你欠高超儿的钱还没还呢吧。']\n",
            "['zh']\n",
            "  4% 54/1500 [00:00<00:23, 60.93it/s]T2S Decoding EOS [103 -> 161]\n",
            "  4% 58/1500 [00:00<00:24, 59.53it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.146\t0.838\t0.975\t0.324\n",
            "实际输入的参考文本: 我就一直学嘛，牛仔式，牛仔的连体。\n",
            "实际输入的目标文本: 。我就一直学嘛，牛仔式，牛仔的连体。\n",
            "[]\n",
            "实际输入的目标文本(切句后): 我就一直学嘛，牛仔式，牛仔的连体\n",
            "['我就一直学嘛，牛仔式，牛仔的连体。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 我就一直学嘛，牛仔式，牛仔的连体。\n",
            "[]\n",
            "['我就一直学嘛，牛仔式，牛仔的连体。']\n",
            "['zh']\n",
            "  6% 89/1500 [00:01<00:23, 61.14it/s]T2S Decoding EOS [103 -> 192]\n",
            "  6% 89/1500 [00:01<00:23, 59.45it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.087\t0.058\t1.498\t0.355\n",
            "实际输入的参考文本: 我就一直学嘛，牛仔式，牛仔的连体。\n",
            "实际输入的目标文本: 。好像没有，其实买个牛仔色的也好看。\n",
            "[]\n",
            "实际输入的目标文本(切句后): 好像没有，其实买个牛仔色的也好看\n",
            "['我就一直学嘛，牛仔式，牛仔的连体。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 好像没有，其实买个牛仔色的也好看。\n",
            "[]\n",
            "['好像没有，其实买个牛仔色的也好看。']\n",
            "['zh']\n",
            "  6% 89/1500 [00:02<00:33, 41.86it/s]T2S Decoding EOS [103 -> 192]\n",
            "  6% 89/1500 [00:02<00:34, 40.87it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.135\t0.095\t2.179\t0.441\n",
            "实际输入的参考文本: 我就一直学嘛，牛仔式，牛仔的连体。\n",
            "实际输入的目标文本: 。小说以发生在2005年湘江造纸厂防空洞里的一起谋杀案为引子，用明暗两条线，记述了一段湮没在时光深处的残酷青春，也解密了一桩尘封十三年的悬案。小说中的三起谋杀案设计精巧，将悬念拉满，不同视角下展开的同一细节，配以快节奏的叙事推进，多层次、多维度地满足了读者探究谜案真相的心理需求。\n",
            "[]\n",
            "实际输入的目标文本(切句后): 小说以发生在2005年湘江造纸厂防空洞里的一起谋杀案为引子，用明暗两条线，记述了一段湮没在时光深处的残酷青春，也解密了一桩尘封十三年的悬案\n",
            "小说中的三起谋杀案设计精巧，将悬念拉满，不同视角下展开的同一细节，配以快节奏的叙事推进，多层次、多维度地满足了读者探究谜案真相的心理需求\n",
            "['我就一直学嘛，牛仔式，牛仔的连体。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 小说以发生在2005年湘江造纸厂防空洞里的一起谋杀案为引子，用明暗两条线，记述了一段湮没在时光深处的残酷青春，也解密了一桩尘封十三年的悬案。\n",
            "[]\n",
            "['小说以发生在2005年湘江造纸厂防空洞里的一起谋杀案为引子，用明暗两条线，记述了一段湮没在时光深处的残酷青春，也解密了一桩尘封十三年的悬案。']\n",
            "['zh']\n",
            " 25% 377/1500 [00:06<00:18, 62.31it/s]T2S Decoding EOS [103 -> 485]\n",
            " 25% 382/1500 [00:06<00:18, 60.16it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 小说中的三起谋杀案设计精巧，将悬念拉满，不同视角下展开的同一细节，配以快节奏的叙事推进，多层次、多维度地满足了读者探究谜案真相的心理需求。\n",
            "[]\n",
            "['小说中的三起谋杀案设计精巧，将悬念拉满，不同视角下展开的同一细节，配以快节奏的叙事推进，多层次、多维度地满足了读者探究谜案真相的心理需求。']\n",
            "['zh']\n",
            " 23% 350/1500 [00:07<00:18, 62.47it/s]T2S Decoding EOS [103 -> 455]\n",
            " 23% 352/1500 [00:07<00:23, 48.29it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.090\t7.036\t7.290\t0.584\n",
            "实际输入的参考文本: 我就一直学嘛，牛仔式，牛仔的连体。\n",
            "实际输入的目标文本: 顾小白说，这么看来，通过监控获得突破的可能性比较小，犯罪嫌疑人大概率是徒步进入案发地。周云鹏的案子要从两个方面寻找突破口，第一，找到用来纵火的汽油的来源。现在私家车普及率很高，如果是有车一族，很可能从自己的车里抽取汽油，灌注到可乐瓶内。这种方式很隐蔽，不太好查。如果是无车一族，购买散装汽油必须登记个人信息，这就好查多了。汽油易燃易爆，味道也大，散装状态下不会保存太久，就查三个月内的购买信息。但犯罪嫌疑人为了逃避警方追查，很可能通过非法销售点购买散装汽油；第二，普通人狩猎，是为了满足口腹之欲，或者为了贩卖盈利。但周云鹏是富豪，想吃什么野物都可以买到，他狩猎应该仅仅是出于爱好。有这种爱好的人在富豪圈子里有不少。周云鹏去狩猎很可能约了同伴，这个消失的同伴作案嫌疑很大。要了解周云鹏的狩猎圈子，找到跟他有相同爱好的人，逐一排查。\n",
            "[]\n",
            "实际输入的目标文本(切句后): 顾小白说，这么看来，通过监控获得突破的可能性比较小，犯罪嫌疑人大概率是徒步进入案发地\n",
            "周云鹏的案子要从两个方面寻找突破口，第一，找到用来纵火的汽油的来源\n",
            "现在私家车普及率很高，如果是有车一族，很可能从自己的车里抽取汽油，灌注到可乐瓶内\n",
            "这种方式很隐蔽，不太好查\n",
            "如果是无车一族，购买散装汽油必须登记个人信息，这就好查多了\n",
            "汽油易燃易爆，味道也大，散装状态下不会保存太久，就查三个月内的购买信息\n",
            "但犯罪嫌疑人为了逃避警方追查，很可能通过非法销售点购买散装汽油；第二，普通人狩猎，是为了满足口腹之欲，或者为了贩卖盈利\n",
            "但周云鹏是富豪，想吃什么野物都可以买到，他狩猎应该仅仅是出于爱好\n",
            "有这种爱好的人在富豪圈子里有不少\n",
            "周云鹏去狩猎很可能约了同伴，这个消失的同伴作案嫌疑很大\n",
            "要了解周云鹏的狩猎圈子，找到跟他有相同爱好的人，逐一排查\n",
            "['我就一直学嘛，牛仔式，牛仔的连体。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 顾小白说，这么看来，通过监控获得突破的可能性比较小，犯罪嫌疑人大概率是徒步进入案发地。\n",
            "[]\n",
            "['顾小白说，这么看来，通过监控获得突破的可能性比较小，犯罪嫌疑人大概率是徒步进入案发地。']\n",
            "['zh']\n",
            " 13% 202/1500 [00:03<00:20, 62.39it/s]T2S Decoding EOS [103 -> 310]\n",
            " 14% 207/1500 [00:03<00:21, 60.81it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 周云鹏的案子要从两个方面寻找突破口，第一，找到用来纵火的汽油的来源。\n",
            "[]\n",
            "['周云鹏的案子要从两个方面寻找突破口，第一，找到用来纵火的汽油的来源。']\n",
            "['zh']\n",
            " 11% 170/1500 [00:04<00:34, 39.04it/s]T2S Decoding EOS [103 -> 276]\n",
            " 12% 173/1500 [00:04<00:31, 42.09it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 现在私家车普及率很高，如果是有车一族，很可能从自己的车里抽取汽油，灌注到可乐瓶内。\n",
            "[]\n",
            "['现在私家车普及率很高，如果是有车一族，很可能从自己的车里抽取汽油，灌注到可乐瓶内。']\n",
            "['zh']\n",
            " 14% 212/1500 [00:03<00:20, 63.29it/s]T2S Decoding EOS [103 -> 317]\n",
            " 14% 214/1500 [00:03<00:21, 61.17it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 这种方式很隐蔽，不太好查。\n",
            "[]\n",
            "['这种方式很隐蔽，不太好查。']\n",
            "['zh']\n",
            "  4% 55/1500 [00:00<00:22, 64.69it/s]T2S Decoding EOS [103 -> 164]\n",
            "  4% 61/1500 [00:00<00:23, 62.32it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 如果是无车一族，购买散装汽油必须登记个人信息，这就好查多了。\n",
            "[]\n",
            "['如果是无车一族，购买散装汽油必须登记个人信息，这就好查多了。']\n",
            "['zh']\n",
            " 10% 146/1500 [00:02<00:22, 61.43it/s]T2S Decoding EOS [103 -> 253]\n",
            " 10% 150/1500 [00:02<00:21, 61.46it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 汽油易燃易爆，味道也大，散装状态下不会保存太久，就查三个月内的购买信息。\n",
            "[]\n",
            "['汽油易燃易爆，味道也大，散装状态下不会保存太久，就查三个月内的购买信息。']\n",
            "['zh']\n",
            " 11% 165/1500 [00:03<00:32, 40.68it/s]T2S Decoding EOS [103 -> 272]\n",
            " 11% 169/1500 [00:03<00:26, 50.10it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 但犯罪嫌疑人为了逃避警方追查，很可能通过非法销售点购买散装汽油；第二，普通人狩猎，是为了满足口腹之欲，或者为了贩卖盈利。\n",
            "[]\n",
            "['但犯罪嫌疑人为了逃避警方追查，很可能通过非法销售点购买散装汽油；第二，普通人狩猎，是为了满足口腹之欲，或者为了贩卖盈利。']\n",
            "['zh']\n",
            " 21% 318/1500 [00:05<00:20, 58.08it/s]T2S Decoding EOS [103 -> 422]\n",
            " 21% 319/1500 [00:05<00:22, 53.35it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 但周云鹏是富豪，想吃什么野物都可以买到，他狩猎应该仅仅是出于爱好。\n",
            "[]\n",
            "['但周云鹏是富豪，想吃什么野物都可以买到，他狩猎应该仅仅是出于爱好。']\n",
            "['zh']\n",
            "  9% 130/1500 [00:02<00:22, 60.79it/s]T2S Decoding EOS [103 -> 238]\n",
            "  9% 135/1500 [00:02<00:23, 58.69it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 有这种爱好的人在富豪圈子里有不少。\n",
            "[]\n",
            "['有这种爱好的人在富豪圈子里有不少。']\n",
            "['zh']\n",
            "  5% 75/1500 [00:01<00:23, 61.49it/s]T2S Decoding EOS [103 -> 178]\n",
            "  5% 75/1500 [00:01<00:23, 60.64it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 周云鹏去狩猎很可能约了同伴，这个消失的同伴作案嫌疑很大。\n",
            "[]\n",
            "['周云鹏去狩猎很可能约了同伴，这个消失的同伴作案嫌疑很大。']\n",
            "['zh']\n",
            "  9% 131/1500 [00:02<00:34, 39.73it/s]T2S Decoding EOS [103 -> 236]\n",
            "  9% 133/1500 [00:02<00:30, 45.16it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 要了解周云鹏的狩猎圈子，找到跟他有相同爱好的人，逐一排查。\n",
            "[]\n",
            "['要了解周云鹏的狩猎圈子，找到跟他有相同爱好的人，逐一排查。']\n",
            "['zh']\n",
            "  9% 135/1500 [00:02<00:23, 57.58it/s]T2S Decoding EOS [103 -> 239]\n",
            "  9% 136/1500 [00:02<00:27, 48.96it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.085\t37.323\t2.779\t0.620\n",
            "实际输入的参考文本: 我就一直学嘛，牛仔式，牛仔的连体。\n",
            "实际输入的目标文本: 顾小白记得很清楚，江蓝是国庆节后才从乌龙中学转到纸厂子弟学校来的，那一天是二〇〇二年十月八日，微雨。看到顾小白还有些蒙圈，梁斌就给他科普了一下生理知识：即使江蓝和孟海认识第一天就发生性关系并怀孕，到十二月二十二日也不可能怀孕十五周。十三年前，顾小白还不谙男女之事，没意识到这个问题，对病历上写的怀孕时间他也没有留意。梁斌说，当年参与查案的人都是男同志，对怀孕时间的推算都是门外汉，忽略了这个重要细节\n",
            "[]\n",
            "实际输入的目标文本(切句后): 顾小白记得很清楚，江蓝是国庆节后才从乌龙中学转到纸厂子弟学校来的，那一天是二〇〇二年十月八日，微雨\n",
            "看到顾小白还有些蒙圈，梁斌就给他科普了一下生理知识：即使江蓝和孟海认识第一天就发生性关系并怀孕，到十二月二十二日也不可能怀孕十五周\n",
            "十三年前，顾小白还不谙男女之事，没意识到这个问题，对病历上写的怀孕时间他也没有留意\n",
            "梁斌说，当年参与查案的人都是男同志，对怀孕时间的推算都是门外汉，忽略了这个重要细节\n",
            "['我就一直学嘛，牛仔式，牛仔的连体。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 顾小白记得很清楚，江蓝是国庆节后才从乌龙中学转到纸厂子弟学校来的，那一天是二〇〇二年十月八日，微雨。\n",
            "[]\n",
            "['顾小白记得很清楚，江蓝是国庆节后才从乌龙中学转到纸厂子弟学校来的，那一天是二〇〇二年十月八日，微雨。']\n",
            "['zh']\n",
            " 18% 264/1500 [00:04<00:20, 61.42it/s]T2S Decoding EOS [103 -> 372]\n",
            " 18% 269/1500 [00:04<00:21, 58.58it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 看到顾小白还有些蒙圈，梁斌就给他科普了一下生理知识：即使江蓝和孟海认识第一天就发生性关系并怀孕，到十二月二十二日也不可能怀孕十五周。\n",
            "[]\n",
            "['看到顾小白还有些蒙圈，梁斌就给他科普了一下生理知识：即使江蓝和孟海认识第一天就发生性关系并怀孕，到十二月二十二日也不可能怀孕十五周。']\n",
            "['zh']\n",
            " 23% 342/1500 [00:05<00:25, 45.90it/s]T2S Decoding EOS [103 -> 447]\n",
            " 23% 344/1500 [00:05<00:20, 57.36it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 十三年前，顾小白还不谙男女之事，没意识到这个问题，对病历上写的怀孕时间他也没有留意。\n",
            "[]\n",
            "['十三年前，顾小白还不谙男女之事，没意识到这个问题，对病历上写的怀孕时间他也没有留意。']\n",
            "['zh']\n",
            " 13% 201/1500 [00:04<00:24, 52.83it/s]T2S Decoding EOS [103 -> 309]\n",
            " 14% 206/1500 [00:04<00:28, 45.74it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "实际输入的目标文本(每句): 梁斌说，当年参与查案的人都是男同志，对怀孕时间的推算都是门外汉，忽略了这个重要细节。\n",
            "[]\n",
            "['梁斌说，当年参与查案的人都是男同志，对怀孕时间的推算都是门外汉，忽略了这个重要细节。']\n",
            "['zh']\n",
            " 14% 205/1500 [00:03<00:22, 58.37it/s]T2S Decoding EOS [103 -> 310]\n",
            " 14% 207/1500 [00:03<00:21, 59.36it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.196\t17.512\t3.488\t0.361\n",
            "实际输入的参考文本: 我就一直学嘛，牛仔式，牛仔的连体。\n",
            "实际输入的目标文本: 上交中网协11%奖金，扣除其他费用，郑钦文805万奖金还剩下多少？\n",
            "[]\n",
            "实际输入的目标文本(切句后): 上交中网协11%奖金，扣除其他费用，郑钦文805万奖金还剩下多少？\n",
            "['我就一直学嘛，牛仔式，牛仔的连体。']\n",
            "['zh']\n",
            "实际输入的目标文本(每句): 上交中网协11%奖金，扣除其他费用，郑钦文805万奖金还剩下多少？\n",
            "[]\n",
            "['上交中网协11%奖金，扣除其他费用，郑钦文805万奖金还剩下多少？']\n",
            "['zh']\n",
            " 13% 196/1500 [00:04<00:21, 60.28it/s]T2S Decoding EOS [103 -> 299]\n",
            " 13% 196/1500 [00:04<00:26, 48.63it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "0.160\t0.104\t4.032\t0.627\n"
          ]
        }
      ]
    }
  ]
}